{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sauddy/DPCNet/blob/main/DPCNet_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Av3XMdEU-5_"
   },
   "source": [
    "## DPNNet-RT -- 22 October 2022 (Colab Compatible)\n",
    "\n",
    "In this notebook we are using the data from the entire 700 simulations:\n",
    "\n",
    "This notebook is created to build a ML model that can Classify the number of hidden planets and Predict the \n",
    "corresponding planet mass for each of the planet from the protoplanetary disk images directly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-5y_R0ILrPP"
   },
   "source": [
    "#####    Please note this version of the code is compatible with Google colab \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37418,
     "status": "ok",
     "timestamp": 1657722963014,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "QG8TXKEFVeok",
    "outputId": "f8ebb99b-6a55-4997-cbf1-176ef13b2b11"
   },
   "outputs": [],
   "source": [
    "####mount the drive if running from Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3CDNlzfU-6A"
   },
   "source": [
    "### IDEA behind this notebook : \n",
    "#### Author : Sayantan \n",
    "#### Created : 3 Feb 2022\n",
    "#### This notebook is adopted from the DPNNet-2.0\n",
    "\n",
    "##### Updated1: 24 Feb 2022 to include multi-outputs\n",
    "##### Updated2: 22 October 2022 to include the all the date from the 700 FARGO3D calculations\n",
    "\n",
    "This notebook is developed to train the Model with RT images:\n",
    "We want to perform the following set of tasks\n",
    "\n",
    " P.S.This is a modular notebook that does the following:\n",
    " 1. Import all the customized Modules from Modules_DPNNet \n",
    " 2. For data processing we use RTdata_processing.py script \n",
    " 3. A functional module to call the different networks independently. (deep_models.py, other_cnn.py)\n",
    " 4. On October 2022, we are updating this notebook. We can now acess the complete data but shall choose randomly from the images\n",
    " 5. Still only considering the axysymmetric images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7338,
     "status": "ok",
     "timestamp": 1657722970345,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "hyZWiknOU-6A",
    "outputId": "4d843a4b-c0ca-44d2-9a54-ca21f2a1b8e5"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "\n",
    "## Modules to check the performance of the code\n",
    "from time import process_time \n",
    "# !pip install memory_profiler ## When running from Google Colab\n",
    "# import memory_profiler as mem_profile\n",
    "# print('Memory (Before): {}Mb'.format(mem_profile.memory_usage()))\n",
    "\n",
    "\n",
    "## Importing the necessary TesnorFLow modules modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# from tensorflow.python.compiler.mlcompute import mlcompute\n",
    "# mlcompute.set_mlc_device(device_name='gpu')\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import r2_score ## form calcualting the r2 score\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow import keras as k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1657722970656,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "bx9nBvvHLrPR",
    "outputId": "1cbea282-af21-4c66-ee39-57e1a0aa81e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2412,
     "status": "ok",
     "timestamp": 1657722973065,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "h9468NQc2-Cq",
    "outputId": "287efd1b-8f8b-4f68-c362-cafedf843250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are currently using the Modules_DPCNet-RT\n",
      "Creation of the directory data_folder failed/ not needed as it already exit\n",
      "Creation of the directory figures failed/ not needed as it already exit\n",
      "Creation of the directory saved_model failed/ not needed as it already exit\n",
      "[INFO] Modules imported\n"
     ]
    }
   ],
   "source": [
    "############ Please provide the same path to the code directory if using Colab################\n",
    "\n",
    "# Path_gdrive= '/content/drive/MyDrive/DPNNet-RT/' ## Comment out this line if using local computer\n",
    "\n",
    "## Importing the Modules from Modules_DPNNet\n",
    "import sys\n",
    "try: ## tries to find the modules in the local directory first\n",
    "  current_directory = os.getcwd()\n",
    "  path = current_directory + '/' # For local computer \n",
    "#   path = '' # For local computer  \n",
    "  sys.path.append(path+'MODULES_DPNNeT')\n",
    "  import data_processing_RT as dp\n",
    "  import deep_models as dm\n",
    "  import other_cnns as ocn\n",
    "\n",
    "########### Folders to save the processed data, files and figures when using Local computer ##############\n",
    "  output_folder_list = ['data_folder','figures','saved_model']\n",
    "  for file in output_folder_list:\n",
    "    try:\n",
    "        os.makedirs(file)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed/ not needed as it already exit\" % file)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s\" % file)\n",
    "  \n",
    "except ModuleNotFoundError:\n",
    "  \n",
    "  # #For Colab use:\n",
    "  # #Point to the path containing the modules in the above section\n",
    "  #(data folder are a directory above the directory containing the notebook)\n",
    "  try:\n",
    "    path = Path_gdrive\n",
    "    print(path)\n",
    "    sys.path.append(path+'MODULES_DPNNeT')\n",
    "    import data_processing_RT as dp\n",
    "    import deep_models as dm\n",
    "    import other_cnns as ocn\n",
    "\n",
    "    ########### Folders to save the processed data, files and figures when using GDRIVE ##############\n",
    "    import os\n",
    "    os.chdir(path)\n",
    "    print(\"Creating the folders\")\n",
    "    !mkdir -p data_folder\n",
    "    !mkdir -p figures ## to save the figures\n",
    "    !mkdir -p figures_paper\n",
    "    !mkdir -p saved_model\n",
    "  except ModuleNotFoundError:\n",
    "    print(\"The path to the modules is incorrect-- Provide current path\")\n",
    "\n",
    "print(\"[INFO] Modules imported\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldbzXokMU-6B"
   },
   "source": [
    "## Creating a csv with simulations params and path to each RT images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1657722973614,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "LGF9QKBiLrPT",
    "outputId": "e2f86397-dc98-4d4d-e32e-d39865db6e3a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Planet_Mass1</th>\n",
       "      <th>Planet_Mass2</th>\n",
       "      <th>Planet_Mass3</th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Stokes</th>\n",
       "      <th>Aspect_Ratio</th>\n",
       "      <th>SigmaSlope</th>\n",
       "      <th>Flaring_Index</th>\n",
       "      <th>Rp1</th>\n",
       "      <th>Rp2</th>\n",
       "      <th>Rp3</th>\n",
       "      <th>a_grain_mic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.19</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002280</td>\n",
       "      <td>0.00154</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>0.0561</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.943</td>\n",
       "      <td>2.16</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.00262</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.160</td>\n",
       "      <td>1.84</td>\n",
       "      <td>3.15</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>0.0496</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.99</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.00187</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.863</td>\n",
       "      <td>2.08</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.0454</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.29</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>0.002530</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.0882</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.851</td>\n",
       "      <td>1.99</td>\n",
       "      <td>3.10</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.130</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.35</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.0711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.966</td>\n",
       "      <td>2.02</td>\n",
       "      <td>3.32</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.0646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.010</td>\n",
       "      <td>1.87</td>\n",
       "      <td>3.25</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Planet_Mass1  Planet_Mass2  Planet_Mass3  Epsilon  Alpha    Stokes  \\\n",
       "0        0.000322       0.00204      0.001210    0.050  0.001  1.570000   \n",
       "1        0.002280       0.00154      0.000963    0.025  0.010  1.570000   \n",
       "2        0.002450       0.00262      0.002460    0.025  0.010  1.570000   \n",
       "3        0.000917       0.00195      0.001050    0.010  0.001  1.570000   \n",
       "4        0.001770       0.00187      0.002210    0.025  0.050  1.570000   \n",
       "..            ...           ...           ...      ...    ...       ...   \n",
       "695      0.002110       0.00000      0.000000    0.025  0.005  0.000157   \n",
       "696      0.002530       0.00000      0.000000    0.025  0.010  0.000157   \n",
       "697      0.000577       0.00000      0.000000    0.025  0.001  0.000157   \n",
       "698      0.001940       0.00000      0.000000    0.025  0.050  0.000157   \n",
       "699      0.002790       0.00000      0.000000    0.050  0.005  0.000157   \n",
       "\n",
       "     Aspect_Ratio  SigmaSlope  Flaring_Index    Rp1   Rp2   Rp3  a_grain_mic  \n",
       "0          0.0432         1.0          0.010  1.010  2.19  3.22       3000.0  \n",
       "1          0.0561         1.0          0.075  0.943  2.16  3.37       1000.0  \n",
       "2          0.0261         1.0          0.075  1.160  1.84  3.15      10000.0  \n",
       "3          0.0496         1.0          0.250  1.100  1.82  2.99      10000.0  \n",
       "4          0.0625         1.0          0.075  0.863  2.08  3.32       3000.0  \n",
       "..            ...         ...            ...    ...   ...   ...          ...  \n",
       "695        0.0454         1.0          0.010  1.100  1.94  3.29        100.0  \n",
       "696        0.0882         1.0          0.075  0.851  1.99  3.10        100.0  \n",
       "697        0.0732         1.0          0.250  1.130  2.00  3.35        100.0  \n",
       "698        0.0711         1.0          0.075  0.966  2.02  3.32        100.0  \n",
       "699        0.0646         1.0          0.250  1.010  1.87  3.25        100.0  \n",
       "\n",
       "[700 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Taking a look at the paramter file \n",
    "parameter_df = dp.load_parameter_csv(path)\n",
    "parameter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Move the images from the original folder to the code directory \n",
    "\n",
    "Only needed initially for moving the images to the working directory--\n",
    "Ones the data is moved there is no need for this code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1657722973615,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "GJh7NLv7U-6B"
   },
   "outputs": [],
   "source": [
    "############ to move the images from the original folder to the code directory ###################\n",
    "## Path to the RT image folder\n",
    "# RT_Folder_Path = \"/Users/sauddy/OneDrive - Iowa State University/images/examples/\"\n",
    "\n",
    "# RT_Folder_Path = \"/home/sauddy3/scratch/DPNNet-RT/radmc3d-2.0/examples/\"\n",
    "\n",
    "\n",
    "# list_RT_path = glob.glob(RT_Folder_Path+ 'RT_A*') ## make a list of all the RT folder where each folder is for each sim\n",
    "# # print(list_RT_path)\n",
    "\n",
    "# list_sorted_RT_path  = sorted(list_RT_path, key=lambda x: int(x.split('/')[7].split('_')[2])) ## sorting the images\n",
    "# print(list_sorted_RT_path)\n",
    "\n",
    "# ############### Moved the images to the code directory for convenience ######\n",
    "# import shutil\n",
    "# try:\n",
    "#     os.mkdir('image_directory_complete')\n",
    "# except OSError:\n",
    "#      print (\"Creation of the directory %s failed/ not needed as it already exit\" % file)\n",
    "# NEW_IMAGE_PATH = 'image_directory_complete'\n",
    "# os.chdir(NEW_IMAGE_PATH)\n",
    "# for index in range(len(list_sorted_RT_path)):\n",
    "#     try:\n",
    "#         os.mkdir('RT_A_'+ str(index+1))\n",
    "#     except OSError:\n",
    "# #          print (\"Creation of the directory %s failed/ not needed as it already exit\" % file)\n",
    "#          pass\n",
    "    \n",
    "#     destination_path = 'RT_A_'+ str(index+1)\n",
    "#     for img in glob.glob(list_sorted_RT_path[index] + \"/images/snu/\"+ \"image_\"+\"*.png\"):\n",
    "#         shutil.copy(img, destination_path)\n",
    "\n",
    "# os.chdir('../')\n",
    "\n",
    "############## END ##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Address to the data folder \n",
    "\n",
    "Genrating the .csv file with the image address to load them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "executionInfo": {
     "elapsed": 31437,
     "status": "ok",
     "timestamp": 1657723005044,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "hB64_GiIlyWP",
    "outputId": "88ce034a-ec64-4ace-bba2-db4ea2d4da46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Importing path for all the RT images\n",
      "[INFO]: Contatinating the paths of all the RT images is now complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Planet_Mass1</th>\n",
       "      <th>Planet_Mass2</th>\n",
       "      <th>Planet_Mass3</th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Stokes</th>\n",
       "      <th>Aspect_Ratio</th>\n",
       "      <th>SigmaSlope</th>\n",
       "      <th>Flaring_Index</th>\n",
       "      <th>Rp1</th>\n",
       "      <th>Rp2</th>\n",
       "      <th>Rp3</th>\n",
       "      <th>a_grain_mic</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77568</th>\n",
       "      <td>22.166667</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.00157</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.851</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.91</td>\n",
       "      <td>300.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78688</th>\n",
       "      <td>363.333333</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.00157</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.909</td>\n",
       "      <td>2.16</td>\n",
       "      <td>3.03</td>\n",
       "      <td>300.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70741</th>\n",
       "      <td>730.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>376.666667</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.00157</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.140</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.96</td>\n",
       "      <td>300.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>703.333333</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>516.666667</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>1.57000</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.150</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.06</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41656</th>\n",
       "      <td>930.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01570</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.931</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9126</th>\n",
       "      <td>560.000000</td>\n",
       "      <td>333.333333</td>\n",
       "      <td>293.333333</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>1.57000</td>\n",
       "      <td>0.0496</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.180</td>\n",
       "      <td>2.03</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32248</th>\n",
       "      <td>873.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.01570</td>\n",
       "      <td>0.0561</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.150</td>\n",
       "      <td>2.08</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9543</th>\n",
       "      <td>50.666667</td>\n",
       "      <td>333.333333</td>\n",
       "      <td>736.666667</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.57000</td>\n",
       "      <td>0.0668</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.897</td>\n",
       "      <td>2.15</td>\n",
       "      <td>3.37</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77915</th>\n",
       "      <td>956.666667</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.00157</td>\n",
       "      <td>0.0496</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.840</td>\n",
       "      <td>2.11</td>\n",
       "      <td>3.29</td>\n",
       "      <td>100.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63161</th>\n",
       "      <td>50.666667</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>433.333333</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.00157</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.110</td>\n",
       "      <td>1.98</td>\n",
       "      <td>3.39</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Planet_Mass1  Planet_Mass2  Planet_Mass3  Epsilon   Alpha   Stokes  \\\n",
       "77568     22.166667     60.000000      0.000000    0.050  0.0010  0.00157   \n",
       "78688    363.333333     60.000000    240.000000    0.010  0.0010  0.00157   \n",
       "70741    730.000000     60.000000    376.666667    0.025  0.0500  0.00157   \n",
       "5447     703.333333     60.000000    516.666667    0.010  0.0050  1.57000   \n",
       "41656    930.000000      0.000000      0.000000    0.050  0.0001  0.01570   \n",
       "...             ...           ...           ...      ...     ...      ...   \n",
       "9126     560.000000    333.333333    293.333333    0.010  0.0050  1.57000   \n",
       "32248    873.333333      0.000000    240.000000    0.050  0.0100  0.01570   \n",
       "9543      50.666667    333.333333    736.666667    0.050  0.0001  1.57000   \n",
       "77915    956.666667     60.000000   1000.000000    0.010  0.0010  0.00157   \n",
       "63161     50.666667     38.000000    433.333333    0.025  0.0100  0.00157   \n",
       "\n",
       "       Aspect_Ratio  SigmaSlope  Flaring_Index    Rp1   Rp2   Rp3  \\\n",
       "77568        0.0732         1.0          0.075  0.851  1.82  2.91   \n",
       "78688        0.0818         1.0          0.010  0.909  2.16  3.03   \n",
       "70741        0.0411         1.0          0.075  1.140  2.15  2.96   \n",
       "5447         0.0732         1.0          0.250  1.150  2.05  3.06   \n",
       "41656        0.0989         1.0          0.010  0.931  2.05  3.46   \n",
       "...             ...         ...            ...    ...   ...   ...   \n",
       "9126         0.0496         1.0          0.010  1.180  2.03  3.23   \n",
       "32248        0.0561         1.0          0.075  1.150  2.08  3.25   \n",
       "9543         0.0668         1.0          0.075  0.897  2.15  3.37   \n",
       "77915        0.0496         1.0          0.250  0.840  2.11  3.29   \n",
       "63161        0.0989         1.0          0.250  1.110  1.98  3.39   \n",
       "\n",
       "       a_grain_mic                                         image_path  \n",
       "77568        300.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "78688        300.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "70741        300.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "5447        1000.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "41656       3000.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "...            ...                                                ...  \n",
       "9126        3000.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "32248       3000.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "9543        3000.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "77915        100.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "63161       1000.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "\n",
       "[15000 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############# Address to the data folder ###################\n",
    "\n",
    "# ## updating the image paths once the transfer is done\n",
    "list_RT_path = glob.glob(path+ 'image_directory_complete/'+ 'RT_A*') ## make a list of all the RT folder where each folder is for each sim\n",
    "\n",
    "## When using the local M1\n",
    "# list_sorted_RT_path  = sorted(list_RT_path, key=lambda x: int(x.split('/')[1].split('_')[2])) ## sorting the images\n",
    "\n",
    "\n",
    "# ## For google colab this needs to be updated\n",
    "list_sorted_RT_path  = sorted(list_RT_path, key=lambda x: int(x.split('/')[6].split('_')[2])) ## sorting the images\n",
    "\n",
    "\n",
    "############## Will be removed once all the images are ready and tested ####\n",
    "# print(list_sorted_RT_path)\n",
    "# df_images_folder_complete=[]\n",
    "# for index in range(len(list_sorted_RT_path)):\n",
    "\n",
    "#     path_image = list_sorted_RT_path[index] ## path to each RT folder\n",
    "\n",
    "#     ## for paths from the RT sim fodlers directly\n",
    "#     list_image_path = glob.glob(path_image + \"/images/snu/\"+ \"image_\"+\"*.png\") ## list of the path to each image in the RT folder\n",
    "#     if len(list_image_path) ==0 :## for updates image paths\n",
    "#         # print(\"Reading images from the updated folder\")\n",
    "#         list_image_path = glob.glob(path_image+'/'+\"*.png\") ## list of the path to each image in the RT folder\n",
    "#         print(\"index ={}, numberofimges ={}\".format(index,len(list_image_path)))\n",
    "#     df_images_folder =pd.DataFrame(list_image_path,columns=[\"image_path\"])\n",
    "#     df_images_folder_complete.append(df_images_folder)\n",
    "    \n",
    "    \n",
    "# df_images_folder_complete = pd.concat(df_images_folder_complete, ignore_index=True, axis=0)\n",
    "# df_images_folder_complete\n",
    "# The idea is to generate a dataframe with the parameters and the path to the images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_complete = dp.create_complete_data_csv(list_sorted_RT_path,path)\n",
    "# data_complete\n",
    "## Removing the nan if any\n",
    "data_complete.isna().sum()  # summing the number of na\n",
    "data_complete= data_complete.dropna()\n",
    "data_complete\n",
    "\n",
    "# data_complete['Planet_Count'] = (data_complete.loc[:, ['Planet_Mass1', 'Planet_Mass2', 'Planet_Mass3']] != 0).sum(axis=1)\n",
    "data_complete = shuffle(data_complete,random_state=42)\n",
    "data_complete= data_complete[:15000] ## Select less numbers of images for quick results\n",
    "data_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDxiZTS9U-6B"
   },
   "source": [
    "## Preparing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3070060,
     "status": "ok",
     "timestamp": 1657726075097,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "agTa7GkoU-6B",
    "outputId": "61a0667a-2edb-4fda-bb4b-247932fbea5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] preparing the normalized data training/testing split...\n",
      "Droping the irrelevant columns \n",
      "INFO: considering multiple planets as output\n",
      "[INFO] Done...\n",
      "[INFO] Loading images from Train data..\n",
      "Train Images are loaded\n",
      "[INFO] Loading images from Test data..\n",
      "Test Images are loaded\n",
      "There are 10837 Train, 1912 Validation and 2250 Test images\n",
      "Total time elapsed = 1000.7474362850189\n"
     ]
    }
   ],
   "source": [
    "## partition the data csv file into training and testing splits using 85% of\n",
    "## the data for training and the remaining 15% for testing\n",
    "split = train_test_split(data_complete, test_size=0.15, random_state=42)\n",
    "(train, test) = split\n",
    "\n",
    "## Save the train and the test data for future use as well.\n",
    "test.to_csv(path+'data_folder/test_dataset.csv')\n",
    "train.to_csv(path+'data_folder/train_dataset.csv')\n",
    "\n",
    "\n",
    "## Generate the Normalized data\n",
    "normed_train_data, normed_test_data, train_labels, test_labels = dp.process_the_disk_attributes(train, test, path,multi_label=True)\n",
    "\n",
    "\n",
    "#### Desired Image resoltuion #####\n",
    "X_res = Y_res =64\n",
    "\n",
    "## Generate the training and the test images \n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "trainImagesX = dp.load_disk_images(train, X_res, Y_res, Type = \"Train\")\n",
    "testImagesX = dp.load_disk_images(test, X_res, Y_res, Type = \"Test\")\n",
    "\n",
    "Validation_split = 0.15 # 15 percent of the training data is used for validation\n",
    "# print('Memory (After Loading): {}Mb'.format(mem_profile.memory_usage()))\n",
    "print('There are {} Train, {} Validation and {} Test images'.format(int((1-Validation_split)*len(normed_train_data)),int(Validation_split*len(normed_train_data)),len(normed_test_data)))## check the numbers in each category\n",
    "end = time.time()\n",
    "print(\"Total time elapsed =\", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1657726075101,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "s-RddkHKLrPV"
   },
   "outputs": [],
   "source": [
    "# train_labels\n",
    "# test\n",
    "# normed_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Checking one image for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1657726075102,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "LPN1iwW6LrPV"
   },
   "outputs": [],
   "source": [
    "# test_ = list_sorted_RT_path[100] + \"/image_\"+\"7.png\"\n",
    "# print(test)\n",
    "# # dimensions for cropping the image\n",
    "# top = 54\n",
    "# left = 102\n",
    "# bottom = 430\n",
    "# right = 480\n",
    "# X_res = Y_res = 512\n",
    "# image = cv2.imread(test_)  \n",
    "# crop_image = image[top:bottom, left:right]\n",
    "# crop_image = cv2.resize(crop_image, (X_res, Y_res)) \n",
    "# crop_image = k.preprocessing.image.img_to_array(crop_image) ## changing to numpy array\n",
    "# datagen = k.preprocessing.image.ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True,rescale= 1.0/255.0)\n",
    "# crop_image = datagen.standardize(np.copy(crop_image))\n",
    "# plt.imshow(crop_image)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1657726075280,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "6jXw5UJ5LrPV",
    "outputId": "341e19e1-032e-4fde-8154-2f398f7cf92d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Planet_Mass1</th>\n",
       "      <th>Planet_Mass2</th>\n",
       "      <th>Planet_Mass3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23606</th>\n",
       "      <td>956.666667</td>\n",
       "      <td>205.333333</td>\n",
       "      <td>44.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60054</th>\n",
       "      <td>220.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23326</th>\n",
       "      <td>900.000000</td>\n",
       "      <td>790.000000</td>\n",
       "      <td>596.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34327</th>\n",
       "      <td>107.333333</td>\n",
       "      <td>333.333333</td>\n",
       "      <td>240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19518</th>\n",
       "      <td>79.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10676</th>\n",
       "      <td>873.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40101</th>\n",
       "      <td>616.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12503</th>\n",
       "      <td>646.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85675</th>\n",
       "      <td>816.666667</td>\n",
       "      <td>706.666667</td>\n",
       "      <td>930.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93269</th>\n",
       "      <td>164.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>321.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12750 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Planet_Mass1  Planet_Mass2  Planet_Mass3\n",
       "23606    956.666667    205.333333     44.000000\n",
       "60054    220.666667      0.000000      0.000000\n",
       "23326    900.000000    790.000000    596.666667\n",
       "34327    107.333333    333.333333    240.000000\n",
       "19518     79.000000      0.000000      0.000000\n",
       "...             ...           ...           ...\n",
       "10676    873.333333      0.000000   1000.000000\n",
       "40101    616.666667      0.000000      0.000000\n",
       "12503    646.666667      0.000000    240.000000\n",
       "85675    816.666667    706.666667    930.000000\n",
       "93269    164.000000     60.000000    321.000000\n",
       "\n",
       "[12750 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.shape(trainImagesX[1])\n",
    "plt.imshow(trainImagesX[5])\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjjWc7sq9amp",
    "tags": []
   },
   "source": [
    "### Preparing the dataset for the classification problem--- \n",
    "\n",
    "##### We are using the one-hot encoding to label the one, two, and three planet system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1657731337855,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "MFng-6cgb1DC"
   },
   "outputs": [],
   "source": [
    "X_train=trainImagesX\n",
    "X_test=testImagesX\n",
    "\n",
    "## For classification we included binary 1 or 0 dependent on if the planet is present or not respectively\n",
    "## Since Planet 1 is always there we do it for planet 2 and 3 only\n",
    "\n",
    "# train['PM1'] = np.where(train['Planet_Mass1']!= 0, 1,0)\n",
    "train['PM2'] = np.where(train['Planet_Mass2']!= 0, 1,0)\n",
    "train['PM3'] = np.where(train['Planet_Mass3']!= 0, 1,0)\n",
    "\n",
    "# test['PM1'] = np.where(test['Planet_Mass1']!= 0, 1, 0)\n",
    "test['PM2'] = np.where(test['Planet_Mass2']!= 0, 1, 0)\n",
    "test['PM3'] = np.where(test['Planet_Mass3']!= 0, 1, 0)\n",
    "\n",
    "Y_train=train[[\"PM2\",\"PM3\"]]\n",
    "Y_test=test[[\"PM2\",\"PM3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1657731338403,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "0pDAjtJc-1M7",
    "outputId": "01e36eff-6c17-42da-9077-a648ea90ca66"
   },
   "outputs": [],
   "source": [
    "# Y_test\n",
    "# Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Different CNN models\n",
    "\n",
    "Will be later moved to the module folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1657737655040,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "ahC7N1pWDOyH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "# modules added for the RESNET50\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.layers import ZeroPadding2D, AveragePooling2D, GlobalMaxPooling2D, Add\n",
    "\n",
    "\n",
    "######## Alexnet\n",
    "\n",
    "\n",
    "def alexnet(width, height, depth, classes=None,regress=False,multi_label=False,classification=False,option=None):\n",
    "    # initialize the input shape and channel dimension, assuming\n",
    "    # TensorFlow/channels-last ordering\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "\n",
    "# define the model input\n",
    "    X_input = Input(shape=inputShape)\n",
    "    # X_input = Input(shape=inputShape)\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "    x = Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), activation='relu', padding=\"same\")(X_input)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\")(x)\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "    x = Conv2D(256, kernel_size=(11, 11), strides=(1, 1), activation='relu', padding=\"same\")(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\")(x)\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "    x = Conv2D(384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\")(x)\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "    x = Conv2D(384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\")(x)\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "    x = Conv2D(256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\")(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\")(x)\n",
    "\n",
    "\n",
    "# Passing it to a Fully Connected layer\n",
    "    x = Flatten()(x)\n",
    "# 1st Fully Connected Layer\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "# Add Dropout to prevent overfitting\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "# 2nd Fully Connected Layer\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "# Add Dropout\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "# 3rd Fully Connected Layer\n",
    "    x = Dense(1000, activation='relu')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "# Add Dropout\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "# # Output Layer\n",
    "#     x = Dense(4)(x)\n",
    "#     # x=Activation(\"relu\")(x)\n",
    "\n",
    "    XX = Flatten()(x)\n",
    "   \n",
    "    if classification==True:\n",
    "      # print(\"A new fully Connexted layer is added\")\n",
    "       ## Adding a Fully connected layer for classification\n",
    "      Xclas = XX\n",
    "      \n",
    "      Xclas = Dense(100, kernel_initializer='he_uniform', activation='relu')(Xclas)\n",
    "      # Xclas = BatchNormalization(axis=1)(Xclas) \n",
    "      print(\"INFO:CNN is used for classification\")\n",
    "      if option ==1:\n",
    "        print(\"Additional INFO:Using Softmax for classification\")\n",
    "        Xclas = Dense(units=classes, activation='softmax')(Xclas) \n",
    "      else:  \n",
    "        print(\"Additional INFO: Using Sigmoid activation for classification\")\n",
    "        Xclas = Dense(units=classes, name='cla',activation='sigmoid')(Xclas)\n",
    "      out_clas = Xclas \n",
    "\n",
    "    if regress == True:\n",
    "      print(\"INFO:CNN is used for regression\")\n",
    "      if multi_label==False:           \n",
    "        Xreg = Dense(1, activation='linear', name='reg', kernel_initializer = glorot_uniform(seed=0))(XX)\n",
    "      if multi_label==True: \n",
    "        ## 28 Feb 2022 added the multi-label output \n",
    "        print(\"INFO: Note Multiple Labels are optimised during regression\")\n",
    "        Xreg = Dense(3, activation='linear', name='reg', kernel_initializer = glorot_uniform(seed=0))(XX)\n",
    "        out_reg = Xreg\n",
    "    # else:\n",
    "    #     print(\"Multi-input is initiated\")\n",
    "    #     X = Dense(4)(X)\n",
    "    #     X = Activation(\"relu\")(X)\n",
    "    \n",
    "\n",
    "    # Create model\n",
    "\n",
    "    if classification == True and regress == True:\n",
    "      print(\"INFO: Performing both regression and classification -- Model Training\")\n",
    "      modelalexnet = Model(inputs = X_input,outputs=[out_reg, out_clas])\n",
    "    elif regress == False:\n",
    "      X = out_clas\n",
    "      print(\"INFO: Classification Model is being trained\")\n",
    "      modelalexnet = Model(inputs = X_input, outputs = X)\n",
    "    elif classification is False:\n",
    "      X = out_reg\n",
    "      print(\"INFO: Regression Model is being trained\")\n",
    "      modelalexnet = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return modelalexnet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############ vgg 16\n",
    "\n",
    "\n",
    "def cnn_vgg(width, height, depth, classes=None,regress=False,multi_label=False,classification=False,option=None):\n",
    "    # initialize the input shape and channel dimension, assuming\n",
    "    # TensorFlow/channels-last ordering\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "\n",
    "    # define the model input\n",
    "    X_input = Input(shape=inputShape)\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(X_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "    \n",
    "    \n",
    "    XX = Flatten()(x)\n",
    "   \n",
    "    if classification==True:\n",
    "      # print(\"A new fully Connexted layer is added\")\n",
    "       ## Adding a Fully connected layer for classification\n",
    "      Xclas = XX\n",
    "      \n",
    "      Xclas = Dense(100, kernel_initializer='he_uniform', activation='relu')(Xclas)\n",
    "      # Xclas = BatchNormalization(axis=1)(Xclas) \n",
    "      print(\"INFO:CNN is used for classification\")\n",
    "      if option ==1:\n",
    "        print(\"Additional INFO:Using Softmax for classification\")\n",
    "        Xclas = Dense(units=classes, activation='softmax')(Xclas) \n",
    "      else:  \n",
    "        print(\"Additional INFO: Using Sigmoid activation for classification\")\n",
    "        Xclas = Dense(units=classes, name='cla',activation='sigmoid')(Xclas)\n",
    "      out_clas = Xclas \n",
    "\n",
    "    if regress == True:\n",
    "      print(\"INFO:CNN is used for regression\")\n",
    "      if multi_label==False:           \n",
    "        Xreg = Dense(1, activation='linear', name='reg', kernel_initializer = glorot_uniform(seed=0))(XX)\n",
    "      if multi_label==True: \n",
    "        ## 28 Feb 2022 added the multi-label output \n",
    "        print(\"INFO: Note Multiple Labels are optimised during regression\")\n",
    "        Xreg = Dense(3, activation='linear', name='reg', kernel_initializer = glorot_uniform(seed=0))(XX)\n",
    "        out_reg = Xreg\n",
    "    # else:\n",
    "    #     print(\"Multi-input is initiated\")\n",
    "    #     X = Dense(4)(X)\n",
    "    #     X = Activation(\"relu\")(X)\n",
    "    \n",
    "\n",
    "    # Create model\n",
    "\n",
    "    if classification == True and regress == True:\n",
    "      print(\"INFO: Performing both regression and classification -- Model Training\")\n",
    "      modelvgg = Model(inputs = X_input,outputs=[out_reg, out_clas])\n",
    "    elif regress == False:\n",
    "      X = out_clas\n",
    "      print(\"INFO: Classification Model is being trained\")\n",
    "      modelvgg = Model(inputs = X_input, outputs = X)\n",
    "    elif classification is False:\n",
    "      X = out_reg\n",
    "      print(\"INFO: Regression Model is being trained\")\n",
    "      modelvgg = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return modelvgg\n",
    "\n",
    "####### RESNET50\n",
    "\n",
    "#identity_block\n",
    "\n",
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    Implementation of the identity block as defined in Figure 3\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value. You'll need this later to add back to the main path. \n",
    "    X_shortcut = X\n",
    "    \n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Second component of main path\n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
    "    X = Add()([X,X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block as defined in Figure 4\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    # First component of main path \n",
    "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a',padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Second component of main path\n",
    "    X = Conv2D(F2, (f, f), strides = (1,1), name = conv_name_base + '2b', padding = 'same', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path\n",
    "    X = Conv2D(F3, (1, 1), strides = (1,1), name = conv_name_base + '2c', padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    ##### SHORTCUT PATH ####\n",
    "    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), name = conv_name_base + '1', padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
    "    X = Add()([X,X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X\n",
    "def Resnet50(width, height, depth,classes=None,regress=False,multi_label=False,classification=False,option=None):\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    inputShape = (height, width, depth)\n",
    "    \n",
    "\n",
    "    # define the model input\n",
    "    X_input = Input(shape=inputShape)\n",
    "\n",
    "\n",
    "\n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    #X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f = 3, filters = [32, 32, 128], stage = 2, block='a', s = 1)\n",
    "    X = identity_block(X, 3, [32, 32, 128], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [32, 32, 128], stage=2, block='c')\n",
    "\n",
    "    # Stage 3\n",
    "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 3, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=3, block='d')\n",
    "\n",
    "    # Stage 4 \n",
    "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 4, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='f')\n",
    "\n",
    "    # Stage 5 \n",
    "    X = convolutional_block(X, f = 3, filters = [256,256, 1024], stage = 5, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [256,256, 1024], stage=5, block='b')\n",
    "    X = identity_block(X, 3, [256,256, 1024], stage=5, block='c')\n",
    "\n",
    "    # AVGPOOL\n",
    "    X = AveragePooling2D(pool_size=(2,2), name='avg_pool')(X)\n",
    "    \n",
    "    ## made the changes on 15 Feb 2021\n",
    "    # output layer\n",
    "    XX = Flatten()(X)\n",
    "    # XX = layers.GlobalAveragePooling2D(name=\"avg_pool\")(X)\n",
    "    XX = Dense(100, kernel_initializer='he_uniform', activation='relu')(XX) \n",
    "   \n",
    "    if classification==True:\n",
    "      # print(\"A new fully Connexted layer is added\")\n",
    "       ## Adding a Fully connected layer for classification\n",
    "      Xclas = XX\n",
    "      Xclas = BatchNormalization(axis=1)(Xclas)   \n",
    "      top_dropout_rate = 0.30\n",
    "      # Xclas = layers.Dropout(top_dropout_rate, name=\"top_dropout1\")(Xclas)  \n",
    "      Xclas = Dense(50, kernel_initializer='he_uniform', activation='relu')(Xclas)\n",
    "      Xclas = BatchNormalization(axis=1)(Xclas)\n",
    "      # top_dropout_rate = 0.20\n",
    "      # Xclas = layers.Dropout(top_dropout_rate, name=\"top_dropout2\")(Xclas)\n",
    "      Xclas = Dense(20, kernel_initializer='he_uniform', activation='relu')(Xclas)\n",
    "      Xclas = BatchNormalization(axis=1)(Xclas)\n",
    "        \n",
    "      print(\"INFO:CNN is used for classification\")\n",
    "      if option ==1:\n",
    "        print(\"Additional INFO:Using Softmax for classification\")\n",
    "        Xclas = Dense(units=classes, activation='softmax')(Xclas) \n",
    "      else:  \n",
    "        print(\"Additional INFO: Using Sigmoid activation for classification\")\n",
    "        Xclas = Dense(units=classes, name='cla',activation='sigmoid')(Xclas)\n",
    "      out_clas = Xclas \n",
    "\n",
    "    if regress == True:\n",
    "      print(\"INFO:CNN is used for regression\")\n",
    "      if multi_label==False:           \n",
    "        Xreg = Dense(1, activation='linear', name='reg', kernel_initializer = glorot_uniform(seed=0))(XX)\n",
    "      if multi_label==True: \n",
    "        ## 28 Feb 2022 added the multi-label output \n",
    "        print(\"INFO: Note Multiple Labels are optimised during regression\")\n",
    "        Xreg = Dense(3, activation='linear', name='reg', kernel_initializer = glorot_uniform(seed=0))(XX)\n",
    "        out_reg = Xreg\n",
    "    # else:\n",
    "    #     print(\"Multi-input is initiated\")\n",
    "    #     X = Dense(4)(X)\n",
    "    #     X = Activation(\"relu\")(X)\n",
    "    \n",
    "   \n",
    "\n",
    "    #### Create model ###\n",
    "\n",
    "    if classification == True and regress == True:\n",
    "      print(\"INFO: Performing both regression and classification -- Model Training\")\n",
    "      modelresnet = Model(inputs = X_input,outputs=[out_reg, out_clas])\n",
    "    elif regress == False:\n",
    "      X = out_clas\n",
    "      print(\"INFO: Classification Model is being trained\")\n",
    "      modelresnet = Model(inputs = X_input, outputs = X)\n",
    "    elif classification is False:\n",
    "      X = out_reg\n",
    "      print(\"INFO: Regression Model is being trained\")\n",
    "      modelresnet = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return modelresnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transfer Learning Codes\n",
    "\n",
    "Will be later moved to the Module Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1 \n",
    "from tensorflow.keras.applications import EfficientNetB2, EfficientNetB3\n",
    "from tensorflow.keras.applications import EfficientNetB4, EfficientNetB5 \n",
    "from tensorflow.keras.applications import EfficientNetB6, EfficientNetB7\n",
    "from tensorflow.keras.applications import ResNet50,ResNet50V2\n",
    "\n",
    "\n",
    "def TRANSFERLEARNING(width,height,depth,classes=None,regress=False,multi_label=False,classification=False,option=None,transfer_model= None):\n",
    "\n",
    "    '''\n",
    "    This function introduces the use of Transfer learning to train the DPNNet Model\n",
    "    Here we use the pretrained weights from the Imagenet data set\n",
    "    \n",
    "    Please note this here update the final layers to perform regress or classification\n",
    "    or both regression and classification together\n",
    "    \n",
    "    Input:  1. Dimension of the image [width,height,depth]\n",
    "            2. Number of classsification classes if used for calssification\n",
    "            3. Regress on or off\n",
    "            4. Multilabel -- True for multi-label regression Default == False\n",
    "            5. Classification -- True or False \n",
    "            6. Option is the argument to use either softmax (default) or sigmoid for classification\n",
    "            7. Select the transfer Learning Model from Either 3 RESNET50 or Efficientnet0-7 models\n",
    "            \n",
    "            \n",
    "    Output : Model Build \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ## Input layers\n",
    "    X_input = layers.Input(shape=(X_res, Y_res, depth))\n",
    "    x = X_input\n",
    "    \n",
    "    if transfer_model == None:\n",
    "        transfer_model = EfficientNetB6\n",
    "        print(\"Please Select the Network that will be used-- By Default {} will be used\".format(str.lower(transfer_model.__name__)))\n",
    "    \n",
    "    #### Selecting the model along with the pretrained weights\n",
    "    mod_name = transfer_model.__name__\n",
    "    print(\"INFO: Keras Model used is {}\".format(mod_name))\n",
    "    print(\"Loading weights for {} since this notebook is not connected to internet\".format(str.lower(mod_name)))\n",
    "    \n",
    "    try:\n",
    "        model = transfer_model(include_top=False,input_tensor=x, weights='preloaded_weights/'+str.lower(mod_name)+'_notop.h5')\n",
    "    except ValueError:\n",
    "        model = transfer_model(include_top=False,input_tensor=x, weights='preloaded_weights/'+str.lower(mod_name)+'_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "    \n",
    "    \n",
    "    # Freeze the pretrained weights\n",
    "    model.trainable = True\n",
    "\n",
    "    # Rebuild top\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    top_dropout_rate = 0.2\n",
    "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "\n",
    "    XX = Flatten()(x)\n",
    "    XX = Dense(100, kernel_initializer='he_uniform', activation='relu')(XX) \n",
    "    # outputs = layers.Dense(classes, activation=\"softmax\", name=\"pred\")(x)\n",
    "\n",
    "\n",
    "    if classification==True:\n",
    "      # print(\"A new fully Connexted layer is added\")\n",
    "       ## Adding a Fully connected layer for classification\n",
    "      Xclas = XX\n",
    "      # Xclas = BatchNormalization(axis=1)(Xclas)   \n",
    "      top_dropout_rate = 0.30\n",
    "      # Xclas = layers.Dropout(top_dropout_rate, name=\"top_dropout1\")(Xclas)  \n",
    "      Xclas = Dense(50, kernel_initializer='he_uniform', activation='relu')(Xclas)\n",
    "      Xclas = BatchNormalization(axis=1)(Xclas)\n",
    "      # top_dropout_rate = 0.20\n",
    "      # Xclas = layers.Dropout(top_dropout_rate, name=\"top_dropout2\")(Xclas)\n",
    "      Xclas = Dense(20, kernel_initializer='he_uniform', activation='relu')(Xclas)\n",
    "      Xclas = BatchNormalization(axis=1)(Xclas)\n",
    "        \n",
    "      print(\"INFO:CNN is used for classification\")\n",
    "      if option ==1:\n",
    "          print(\"Additional INFO:Using Softmax for classification\")\n",
    "          Xclas = Dense(units=classes,name='cla',activation='softmax')(Xclas) \n",
    "      else:  \n",
    "          print(\"Additional INFO: Using Sigmoid activation for classification\")\n",
    "          Xclas = Dense(units=classes, name='cla',activation='sigmoid')(Xclas)\n",
    "          out_clas = Xclas \n",
    "\n",
    "\n",
    "    if regress == True:\n",
    "      print(\"INFO:CNN is used for regression\")\n",
    "      if multi_label==False:           \n",
    "        Xreg = Dense(1, activation='linear', name='reg', kernel_initializer = glorot_uniform(seed=0))(XX)\n",
    "      if multi_label==True: \n",
    "        ## 28 Feb 2022 added the multi-label output \n",
    "        print(\"INFO: Note Multiple Labels are optimised during regression\")\n",
    "        Xreg = Dense(3, activation='linear', name='reg', kernel_initializer = glorot_uniform(seed=0))(XX)\n",
    "        out_reg = Xreg\n",
    "    \n",
    "\n",
    "    ### Create model\n",
    "\n",
    "    if classification == True and regress == True:\n",
    "      print(\"INFO: Performing both regression and classification -- Model Training\")\n",
    "      modelNEWCNN = Model(inputs = X_input,outputs=[out_reg, out_clas])\n",
    "    elif regress == False:\n",
    "      X = out_clas\n",
    "      print(\"INFO: Classification Model is being trained\")\n",
    "      modelNEWCNN = Model(inputs = X_input, outputs = X)\n",
    "    elif classification is False:\n",
    "      X = out_reg\n",
    "      print(\"INFO: Regression Model is being trained\")\n",
    "      modelNEWCNN = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return modelNEWCNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYGYS3pgU-6C",
    "tags": []
   },
   "source": [
    "## Training the CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1657737657204,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "vkIRL4NmU-6C"
   },
   "outputs": [],
   "source": [
    "## Hyper-Parameter to define\n",
    "batch_size = 200 ## 20 was for regression ## the best was for 200 last run\n",
    "valid_batch_size = 200\n",
    "epochs=200 ## best was 100\n",
    "init_lr = 1e-5 # 1e-5 (works for regression)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "# early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.2, patience=20, verbose=1, mode='min',restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lW02M4WU-6C",
    "outputId": "3ccb437f-c01f-4f29-c467-8b68f18545a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Currently training using the TL NETWORK with regression = False and classification = True\n",
      "INFO: Keras Model used is ResNet50\n",
      "Loading weights for resnet50 since this notebook is not connected to internet\n",
      "INFO:CNN is used for classification\n",
      "Additional INFO: Using Sigmoid activation for classification\n",
      "INFO: Classification Model is being trained\n",
      "Epoch 1/200\n",
      "55/55 [==============================] - 20s 196ms/step - loss: 0.6959 - accuracy: 0.4716 - val_loss: 1.8914 - val_accuracy: 0.8876\n",
      "Epoch 2/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.4893 - accuracy: 0.5014 - val_loss: 1.8495 - val_accuracy: 0.8876\n",
      "Epoch 3/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.4138 - accuracy: 0.5232 - val_loss: 1.8238 - val_accuracy: 0.8876\n",
      "Epoch 4/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.3683 - accuracy: 0.5383 - val_loss: 2.4403 - val_accuracy: 0.8840\n",
      "Epoch 5/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.3357 - accuracy: 0.5477 - val_loss: 2.9860 - val_accuracy: 0.8813\n",
      "Epoch 6/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.3072 - accuracy: 0.5595 - val_loss: 2.7260 - val_accuracy: 0.8719\n",
      "Epoch 7/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.2830 - accuracy: 0.5676 - val_loss: 2.3563 - val_accuracy: 0.8108\n",
      "Epoch 8/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.2604 - accuracy: 0.5766 - val_loss: 1.1313 - val_accuracy: 0.4349\n",
      "Epoch 9/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.2477 - accuracy: 0.5837 - val_loss: 0.8619 - val_accuracy: 0.2457\n",
      "Epoch 10/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.2327 - accuracy: 0.5895 - val_loss: 0.7540 - val_accuracy: 0.2290\n",
      "Epoch 11/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.2158 - accuracy: 0.5936 - val_loss: 0.7314 - val_accuracy: 0.3570\n",
      "Epoch 12/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.2042 - accuracy: 0.5979 - val_loss: 0.7473 - val_accuracy: 0.3910\n",
      "Epoch 13/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1944 - accuracy: 0.6018 - val_loss: 0.7034 - val_accuracy: 0.4177\n",
      "Epoch 14/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1870 - accuracy: 0.6039 - val_loss: 0.6208 - val_accuracy: 0.4030\n",
      "Epoch 15/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1761 - accuracy: 0.6135 - val_loss: 0.5488 - val_accuracy: 0.4391\n",
      "Epoch 16/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1668 - accuracy: 0.6191 - val_loss: 0.4770 - val_accuracy: 0.4506\n",
      "Epoch 17/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1589 - accuracy: 0.6193 - val_loss: 0.3999 - val_accuracy: 0.4767\n",
      "Epoch 18/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1500 - accuracy: 0.6285 - val_loss: 0.3403 - val_accuracy: 0.5761\n",
      "Epoch 19/200\n",
      "55/55 [==============================] - 8s 147ms/step - loss: 0.1449 - accuracy: 0.6390 - val_loss: 0.2614 - val_accuracy: 0.5876\n",
      "Epoch 20/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1394 - accuracy: 0.6414 - val_loss: 0.2217 - val_accuracy: 0.6419\n",
      "Epoch 21/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1363 - accuracy: 0.6422 - val_loss: 0.2037 - val_accuracy: 0.6477\n",
      "Epoch 22/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1299 - accuracy: 0.6499 - val_loss: 0.1822 - val_accuracy: 0.7109\n",
      "Epoch 23/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1282 - accuracy: 0.6522 - val_loss: 0.1759 - val_accuracy: 0.7130\n",
      "Epoch 24/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1184 - accuracy: 0.6544 - val_loss: 0.1735 - val_accuracy: 0.6979\n",
      "Epoch 25/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1159 - accuracy: 0.6596 - val_loss: 0.1660 - val_accuracy: 0.7423\n",
      "Epoch 26/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1120 - accuracy: 0.6661 - val_loss: 0.1679 - val_accuracy: 0.7282\n",
      "Epoch 27/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1070 - accuracy: 0.6741 - val_loss: 0.1654 - val_accuracy: 0.7402\n",
      "Epoch 28/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1055 - accuracy: 0.6754 - val_loss: 0.1630 - val_accuracy: 0.7491\n",
      "Epoch 29/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.1022 - accuracy: 0.6904 - val_loss: 0.1609 - val_accuracy: 0.7470\n",
      "Epoch 30/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0970 - accuracy: 0.6930 - val_loss: 0.1609 - val_accuracy: 0.7642\n",
      "Epoch 31/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0944 - accuracy: 0.7026 - val_loss: 0.1608 - val_accuracy: 0.7470\n",
      "Epoch 32/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0960 - accuracy: 0.6981 - val_loss: 0.1803 - val_accuracy: 0.7392\n",
      "Epoch 33/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0915 - accuracy: 0.6939 - val_loss: 0.1663 - val_accuracy: 0.7716\n",
      "Epoch 34/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0884 - accuracy: 0.7095 - val_loss: 0.1627 - val_accuracy: 0.7763\n",
      "Epoch 35/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0846 - accuracy: 0.7186 - val_loss: 0.1514 - val_accuracy: 0.7841\n",
      "Epoch 36/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0777 - accuracy: 0.7125 - val_loss: 0.1476 - val_accuracy: 0.7862\n",
      "Epoch 37/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0764 - accuracy: 0.7158 - val_loss: 0.1535 - val_accuracy: 0.8008\n",
      "Epoch 38/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0855 - accuracy: 0.7199 - val_loss: 0.1457 - val_accuracy: 0.7919\n",
      "Epoch 39/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0828 - accuracy: 0.7224 - val_loss: 0.1520 - val_accuracy: 0.7925\n",
      "Epoch 40/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0745 - accuracy: 0.7263 - val_loss: 0.1569 - val_accuracy: 0.8024\n",
      "Epoch 41/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0781 - accuracy: 0.7292 - val_loss: 0.1583 - val_accuracy: 0.7778\n",
      "Epoch 42/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0711 - accuracy: 0.7338 - val_loss: 0.1511 - val_accuracy: 0.8092\n",
      "Epoch 43/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0659 - accuracy: 0.7434 - val_loss: 0.1506 - val_accuracy: 0.8238\n",
      "Epoch 44/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0660 - accuracy: 0.7480 - val_loss: 0.1477 - val_accuracy: 0.8102\n",
      "Epoch 45/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0782 - accuracy: 0.7340 - val_loss: 0.1563 - val_accuracy: 0.7778\n",
      "Epoch 46/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0682 - accuracy: 0.7425 - val_loss: 0.1509 - val_accuracy: 0.7993\n",
      "Epoch 47/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0600 - accuracy: 0.7524 - val_loss: 0.1463 - val_accuracy: 0.8076\n",
      "Epoch 48/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0600 - accuracy: 0.7579 - val_loss: 0.1488 - val_accuracy: 0.8097\n",
      "Epoch 49/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0647 - accuracy: 0.7631 - val_loss: 0.1465 - val_accuracy: 0.8040\n",
      "Epoch 50/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0638 - accuracy: 0.7525 - val_loss: 0.1357 - val_accuracy: 0.8029\n",
      "Epoch 51/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0586 - accuracy: 0.7689 - val_loss: 0.1427 - val_accuracy: 0.8191\n",
      "Epoch 52/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0623 - accuracy: 0.7521 - val_loss: 0.1479 - val_accuracy: 0.7987\n",
      "Epoch 53/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0554 - accuracy: 0.7658 - val_loss: 0.1418 - val_accuracy: 0.8155\n",
      "Epoch 54/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0613 - accuracy: 0.7581 - val_loss: 0.1448 - val_accuracy: 0.8160\n",
      "Epoch 55/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0571 - accuracy: 0.7723 - val_loss: 0.1470 - val_accuracy: 0.8050\n",
      "Epoch 56/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0682 - accuracy: 0.7709 - val_loss: 0.1381 - val_accuracy: 0.8055\n",
      "Epoch 57/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0622 - accuracy: 0.7772 - val_loss: 0.1384 - val_accuracy: 0.8385\n",
      "Epoch 58/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0568 - accuracy: 0.7812 - val_loss: 0.1472 - val_accuracy: 0.8296\n",
      "Epoch 59/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0497 - accuracy: 0.7866 - val_loss: 0.1359 - val_accuracy: 0.8322\n",
      "Epoch 60/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0508 - accuracy: 0.7951 - val_loss: 0.1379 - val_accuracy: 0.8359\n",
      "Epoch 61/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0542 - accuracy: 0.7894 - val_loss: 0.1426 - val_accuracy: 0.8155\n",
      "Epoch 62/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0509 - accuracy: 0.7929 - val_loss: 0.1377 - val_accuracy: 0.8374\n",
      "Epoch 63/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0537 - accuracy: 0.7992 - val_loss: 0.1533 - val_accuracy: 0.8343\n",
      "Epoch 64/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0575 - accuracy: 0.7873 - val_loss: 0.1482 - val_accuracy: 0.7993\n",
      "Epoch 65/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0517 - accuracy: 0.7843 - val_loss: 0.1402 - val_accuracy: 0.8437\n",
      "Epoch 66/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0448 - accuracy: 0.7928 - val_loss: 0.1335 - val_accuracy: 0.8521\n",
      "Epoch 67/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0425 - accuracy: 0.7982 - val_loss: 0.1288 - val_accuracy: 0.8442\n",
      "Epoch 68/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0422 - accuracy: 0.8060 - val_loss: 0.1386 - val_accuracy: 0.8531\n",
      "Epoch 69/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0439 - accuracy: 0.8111 - val_loss: 0.1358 - val_accuracy: 0.8364\n",
      "Epoch 70/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0460 - accuracy: 0.8110 - val_loss: 0.1376 - val_accuracy: 0.8505\n",
      "Epoch 71/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0452 - accuracy: 0.8188 - val_loss: 0.1339 - val_accuracy: 0.8630\n",
      "Epoch 72/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0427 - accuracy: 0.8082 - val_loss: 0.1351 - val_accuracy: 0.8610\n",
      "Epoch 73/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0426 - accuracy: 0.8173 - val_loss: 0.1363 - val_accuracy: 0.8406\n",
      "Epoch 74/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0448 - accuracy: 0.8053 - val_loss: 0.1441 - val_accuracy: 0.8296\n",
      "Epoch 75/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0446 - accuracy: 0.8168 - val_loss: 0.1418 - val_accuracy: 0.8474\n",
      "Epoch 76/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0513 - accuracy: 0.8093 - val_loss: 0.1357 - val_accuracy: 0.8364\n",
      "Epoch 77/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0415 - accuracy: 0.8163 - val_loss: 0.1333 - val_accuracy: 0.8484\n",
      "Epoch 78/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0462 - accuracy: 0.8222 - val_loss: 0.1373 - val_accuracy: 0.8458\n",
      "Epoch 79/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0444 - accuracy: 0.8194 - val_loss: 0.1343 - val_accuracy: 0.8604\n",
      "Epoch 80/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0429 - accuracy: 0.8181 - val_loss: 0.1366 - val_accuracy: 0.8646\n",
      "Epoch 81/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0509 - accuracy: 0.8082 - val_loss: 0.1362 - val_accuracy: 0.8489\n",
      "Epoch 82/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0482 - accuracy: 0.8191 - val_loss: 0.1406 - val_accuracy: 0.8437\n",
      "Epoch 83/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0428 - accuracy: 0.8255 - val_loss: 0.1381 - val_accuracy: 0.8672\n",
      "Epoch 84/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0363 - accuracy: 0.8344 - val_loss: 0.1309 - val_accuracy: 0.8625\n",
      "Epoch 85/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0369 - accuracy: 0.8186 - val_loss: 0.1386 - val_accuracy: 0.8583\n",
      "Epoch 86/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0401 - accuracy: 0.8310 - val_loss: 0.1390 - val_accuracy: 0.8515\n",
      "Epoch 87/200\n",
      "55/55 [==============================] - 8s 147ms/step - loss: 0.0346 - accuracy: 0.8326 - val_loss: 0.1322 - val_accuracy: 0.8630\n",
      "Epoch 88/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0355 - accuracy: 0.8345 - val_loss: 0.1337 - val_accuracy: 0.8573\n",
      "Epoch 89/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0400 - accuracy: 0.8301 - val_loss: 0.1350 - val_accuracy: 0.8536\n",
      "Epoch 90/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0421 - accuracy: 0.8279 - val_loss: 0.1302 - val_accuracy: 0.8667\n",
      "Epoch 91/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0466 - accuracy: 0.8297 - val_loss: 0.1425 - val_accuracy: 0.8657\n",
      "Epoch 92/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0447 - accuracy: 0.8294 - val_loss: 0.1303 - val_accuracy: 0.8510\n",
      "Epoch 93/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0381 - accuracy: 0.8284 - val_loss: 0.1283 - val_accuracy: 0.8568\n",
      "Epoch 94/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0388 - accuracy: 0.8328 - val_loss: 0.1228 - val_accuracy: 0.8552\n",
      "Epoch 95/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0353 - accuracy: 0.8322 - val_loss: 0.1296 - val_accuracy: 0.8615\n",
      "Epoch 96/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0332 - accuracy: 0.8401 - val_loss: 0.1257 - val_accuracy: 0.8808\n",
      "Epoch 97/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0315 - accuracy: 0.8463 - val_loss: 0.1256 - val_accuracy: 0.8772\n",
      "Epoch 98/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0318 - accuracy: 0.8550 - val_loss: 0.1351 - val_accuracy: 0.8834\n",
      "Epoch 99/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0319 - accuracy: 0.8538 - val_loss: 0.1298 - val_accuracy: 0.8693\n",
      "Epoch 100/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0309 - accuracy: 0.8451 - val_loss: 0.1262 - val_accuracy: 0.8709\n",
      "Epoch 101/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0318 - accuracy: 0.8497 - val_loss: 0.1236 - val_accuracy: 0.8766\n",
      "Epoch 102/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0302 - accuracy: 0.8516 - val_loss: 0.1332 - val_accuracy: 0.8704\n",
      "Epoch 103/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0307 - accuracy: 0.8498 - val_loss: 0.1228 - val_accuracy: 0.8777\n",
      "Epoch 104/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0343 - accuracy: 0.8446 - val_loss: 0.1257 - val_accuracy: 0.8672\n",
      "Epoch 105/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0320 - accuracy: 0.8348 - val_loss: 0.1231 - val_accuracy: 0.8677\n",
      "Epoch 106/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0297 - accuracy: 0.8422 - val_loss: 0.1245 - val_accuracy: 0.8740\n",
      "Epoch 107/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0304 - accuracy: 0.8546 - val_loss: 0.1226 - val_accuracy: 0.8740\n",
      "Epoch 108/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0290 - accuracy: 0.8413 - val_loss: 0.1254 - val_accuracy: 0.8808\n",
      "Epoch 109/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0260 - accuracy: 0.8607 - val_loss: 0.1319 - val_accuracy: 0.8672\n",
      "Epoch 110/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0283 - accuracy: 0.8580 - val_loss: 0.1264 - val_accuracy: 0.8829\n",
      "Epoch 111/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0352 - accuracy: 0.8238 - val_loss: 0.1276 - val_accuracy: 0.8500\n",
      "Epoch 112/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0305 - accuracy: 0.8357 - val_loss: 0.1404 - val_accuracy: 0.8745\n",
      "Epoch 113/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0389 - accuracy: 0.8224 - val_loss: 0.1349 - val_accuracy: 0.8714\n",
      "Epoch 114/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0406 - accuracy: 0.8247 - val_loss: 0.1196 - val_accuracy: 0.8819\n",
      "Epoch 115/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0309 - accuracy: 0.8259 - val_loss: 0.1330 - val_accuracy: 0.8651\n",
      "Epoch 116/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0274 - accuracy: 0.8328 - val_loss: 0.1232 - val_accuracy: 0.8677\n",
      "Epoch 117/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0285 - accuracy: 0.8297 - val_loss: 0.1306 - val_accuracy: 0.8662\n",
      "Epoch 118/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0263 - accuracy: 0.8462 - val_loss: 0.1228 - val_accuracy: 0.8646\n",
      "Epoch 119/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0294 - accuracy: 0.8355 - val_loss: 0.1318 - val_accuracy: 0.8792\n",
      "Epoch 120/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0301 - accuracy: 0.8385 - val_loss: 0.1234 - val_accuracy: 0.8709\n",
      "Epoch 121/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0252 - accuracy: 0.8494 - val_loss: 0.1227 - val_accuracy: 0.8662\n",
      "Epoch 122/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0309 - accuracy: 0.8402 - val_loss: 0.1243 - val_accuracy: 0.8484\n",
      "Epoch 123/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0315 - accuracy: 0.8397 - val_loss: 0.1419 - val_accuracy: 0.8735\n",
      "Epoch 124/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0293 - accuracy: 0.8382 - val_loss: 0.1168 - val_accuracy: 0.8975\n",
      "Epoch 125/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0262 - accuracy: 0.8484 - val_loss: 0.1213 - val_accuracy: 0.8855\n",
      "Epoch 126/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0286 - accuracy: 0.8480 - val_loss: 0.1178 - val_accuracy: 0.8881\n",
      "Epoch 127/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0253 - accuracy: 0.8507 - val_loss: 0.1223 - val_accuracy: 0.8798\n",
      "Epoch 128/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0293 - accuracy: 0.8286 - val_loss: 0.1132 - val_accuracy: 0.8505\n",
      "Epoch 129/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0261 - accuracy: 0.8316 - val_loss: 0.1135 - val_accuracy: 0.8672\n",
      "Epoch 130/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0240 - accuracy: 0.8274 - val_loss: 0.1225 - val_accuracy: 0.8730\n",
      "Epoch 131/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0220 - accuracy: 0.8389 - val_loss: 0.1110 - val_accuracy: 0.8913\n",
      "Epoch 132/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0221 - accuracy: 0.8432 - val_loss: 0.1172 - val_accuracy: 0.8965\n",
      "Epoch 133/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0217 - accuracy: 0.8441 - val_loss: 0.1165 - val_accuracy: 0.8866\n",
      "Epoch 134/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0296 - accuracy: 0.8345 - val_loss: 0.1203 - val_accuracy: 0.8751\n",
      "Epoch 135/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0237 - accuracy: 0.8413 - val_loss: 0.1194 - val_accuracy: 0.8881\n",
      "Epoch 136/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0220 - accuracy: 0.8504 - val_loss: 0.1235 - val_accuracy: 0.8725\n",
      "Epoch 137/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0232 - accuracy: 0.8477 - val_loss: 0.1190 - val_accuracy: 0.8772\n",
      "Epoch 138/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0240 - accuracy: 0.8422 - val_loss: 0.1189 - val_accuracy: 0.8871\n",
      "Epoch 139/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0224 - accuracy: 0.8407 - val_loss: 0.1209 - val_accuracy: 0.8725\n",
      "Epoch 140/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0224 - accuracy: 0.8516 - val_loss: 0.1156 - val_accuracy: 0.8955\n",
      "Epoch 141/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0226 - accuracy: 0.8405 - val_loss: 0.1173 - val_accuracy: 0.8934\n",
      "Epoch 142/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0263 - accuracy: 0.8507 - val_loss: 0.1149 - val_accuracy: 0.9017\n",
      "Epoch 143/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0231 - accuracy: 0.8378 - val_loss: 0.1107 - val_accuracy: 0.8834\n",
      "Epoch 144/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0200 - accuracy: 0.8501 - val_loss: 0.1145 - val_accuracy: 0.8902\n",
      "Epoch 145/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0184 - accuracy: 0.8481 - val_loss: 0.1106 - val_accuracy: 0.8939\n",
      "Epoch 146/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0210 - accuracy: 0.8596 - val_loss: 0.1277 - val_accuracy: 0.8751\n",
      "Epoch 147/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0187 - accuracy: 0.8524 - val_loss: 0.1319 - val_accuracy: 0.8860\n",
      "Epoch 148/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0196 - accuracy: 0.8520 - val_loss: 0.1350 - val_accuracy: 0.8756\n",
      "Epoch 149/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0228 - accuracy: 0.8582 - val_loss: 0.1311 - val_accuracy: 0.8803\n",
      "Epoch 150/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0245 - accuracy: 0.8457 - val_loss: 0.1269 - val_accuracy: 0.8688\n",
      "Epoch 151/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0275 - accuracy: 0.8379 - val_loss: 0.1383 - val_accuracy: 0.8474\n",
      "Epoch 152/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0279 - accuracy: 0.8241 - val_loss: 0.1336 - val_accuracy: 0.8458\n",
      "Epoch 153/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0287 - accuracy: 0.8168 - val_loss: 0.1208 - val_accuracy: 0.8594\n",
      "Epoch 154/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0228 - accuracy: 0.8301 - val_loss: 0.1150 - val_accuracy: 0.8719\n",
      "Epoch 155/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0230 - accuracy: 0.8250 - val_loss: 0.1300 - val_accuracy: 0.8704\n",
      "Epoch 156/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0174 - accuracy: 0.8417 - val_loss: 0.1236 - val_accuracy: 0.8918\n",
      "Epoch 157/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0189 - accuracy: 0.8405 - val_loss: 0.1214 - val_accuracy: 0.8845\n",
      "Epoch 158/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0212 - accuracy: 0.8365 - val_loss: 0.1190 - val_accuracy: 0.8955\n",
      "Epoch 159/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0205 - accuracy: 0.8414 - val_loss: 0.1198 - val_accuracy: 0.8897\n",
      "Epoch 160/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0183 - accuracy: 0.8415 - val_loss: 0.1190 - val_accuracy: 0.8761\n",
      "Epoch 161/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0172 - accuracy: 0.8402 - val_loss: 0.1217 - val_accuracy: 0.8725\n",
      "Epoch 162/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0160 - accuracy: 0.8510 - val_loss: 0.1208 - val_accuracy: 0.8866\n",
      "Epoch 163/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0214 - accuracy: 0.8483 - val_loss: 0.1231 - val_accuracy: 0.8735\n",
      "Epoch 164/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0173 - accuracy: 0.8429 - val_loss: 0.1169 - val_accuracy: 0.8871\n",
      "Epoch 165/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0179 - accuracy: 0.8458 - val_loss: 0.1140 - val_accuracy: 0.8876\n",
      "Epoch 166/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0158 - accuracy: 0.8510 - val_loss: 0.1106 - val_accuracy: 0.8803\n",
      "Epoch 167/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0154 - accuracy: 0.8592 - val_loss: 0.1150 - val_accuracy: 0.8808\n",
      "Epoch 168/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0155 - accuracy: 0.8476 - val_loss: 0.1130 - val_accuracy: 0.8923\n",
      "Epoch 169/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0180 - accuracy: 0.8641 - val_loss: 0.1106 - val_accuracy: 0.8923\n",
      "Epoch 170/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0156 - accuracy: 0.8411 - val_loss: 0.1143 - val_accuracy: 0.8871\n",
      "Epoch 171/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0164 - accuracy: 0.8310 - val_loss: 0.1152 - val_accuracy: 0.8657\n",
      "Epoch 172/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0193 - accuracy: 0.8472 - val_loss: 0.1135 - val_accuracy: 0.8667\n",
      "Epoch 173/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0151 - accuracy: 0.8443 - val_loss: 0.1140 - val_accuracy: 0.8840\n",
      "Epoch 174/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0156 - accuracy: 0.8585 - val_loss: 0.1164 - val_accuracy: 0.8813\n",
      "Epoch 175/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0169 - accuracy: 0.8432 - val_loss: 0.1102 - val_accuracy: 0.8829\n",
      "Epoch 176/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0159 - accuracy: 0.8463 - val_loss: 0.1186 - val_accuracy: 0.8834\n",
      "Epoch 177/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0197 - accuracy: 0.8438 - val_loss: 0.1313 - val_accuracy: 0.8824\n",
      "Epoch 178/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0244 - accuracy: 0.8381 - val_loss: 0.1348 - val_accuracy: 0.8766\n",
      "Epoch 179/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0339 - accuracy: 0.8407 - val_loss: 0.1197 - val_accuracy: 0.8824\n",
      "Epoch 180/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0257 - accuracy: 0.8390 - val_loss: 0.1117 - val_accuracy: 0.8610\n",
      "Epoch 181/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0177 - accuracy: 0.8342 - val_loss: 0.1242 - val_accuracy: 0.8641\n",
      "Epoch 182/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0156 - accuracy: 0.8391 - val_loss: 0.1153 - val_accuracy: 0.8787\n",
      "Epoch 183/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0181 - accuracy: 0.8366 - val_loss: 0.1149 - val_accuracy: 0.8740\n",
      "Epoch 184/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0204 - accuracy: 0.8382 - val_loss: 0.1170 - val_accuracy: 0.8604\n",
      "Epoch 185/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0188 - accuracy: 0.8262 - val_loss: 0.1116 - val_accuracy: 0.8677\n",
      "Epoch 186/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0155 - accuracy: 0.8198 - val_loss: 0.1145 - val_accuracy: 0.8547\n",
      "Epoch 187/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0153 - accuracy: 0.8244 - val_loss: 0.1246 - val_accuracy: 0.8557\n",
      "Epoch 188/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0145 - accuracy: 0.8348 - val_loss: 0.1206 - val_accuracy: 0.8714\n",
      "Epoch 189/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0190 - accuracy: 0.8218 - val_loss: 0.1269 - val_accuracy: 0.8505\n",
      "Epoch 190/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0165 - accuracy: 0.8187 - val_loss: 0.1230 - val_accuracy: 0.8730\n",
      "Epoch 191/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0133 - accuracy: 0.8222 - val_loss: 0.1222 - val_accuracy: 0.8641\n",
      "Epoch 192/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0140 - accuracy: 0.8352 - val_loss: 0.1137 - val_accuracy: 0.8834\n",
      "Epoch 193/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0144 - accuracy: 0.8500 - val_loss: 0.1099 - val_accuracy: 0.8803\n",
      "Epoch 194/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0127 - accuracy: 0.8466 - val_loss: 0.1204 - val_accuracy: 0.8693\n",
      "Epoch 195/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0178 - accuracy: 0.8286 - val_loss: 0.1170 - val_accuracy: 0.8583\n",
      "Epoch 196/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0149 - accuracy: 0.8416 - val_loss: 0.1197 - val_accuracy: 0.8772\n",
      "Epoch 197/200\n",
      "55/55 [==============================] - 8s 144ms/step - loss: 0.0195 - accuracy: 0.8399 - val_loss: 0.1272 - val_accuracy: 0.8557\n",
      "Epoch 198/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0158 - accuracy: 0.8393 - val_loss: 0.1094 - val_accuracy: 0.8740\n",
      "Epoch 199/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0189 - accuracy: 0.8350 - val_loss: 0.1429 - val_accuracy: 0.8515\n",
      "Epoch 200/200\n",
      "55/55 [==============================] - 8s 143ms/step - loss: 0.0151 - accuracy: 0.8350 - val_loss: 0.1232 - val_accuracy: 0.8751\n"
     ]
    }
   ],
   "source": [
    "## Select the Network type\n",
    "\n",
    "# NETWORK = \"Vanilla\" ## Cannot be uses at the moment\n",
    "# NETWORK = \"ALEXNET\"\n",
    "# NETWORK = \"VGG\"\n",
    "# NETWORK = \"RESNET50\"\n",
    "\n",
    "                                ### When using Trasnfer Learning ######################\n",
    "NETWORK = \"TL\"\n",
    "transfer_model=ResNet50\n",
    "# transfer_model=EfficientNetB3\n",
    "\n",
    "## Select the kind of Traning ## Both can be selected as well\n",
    "REG = False #True  ## When choosing regression\n",
    "CLA = True #False #True  ## When Choosing Clasiffication\n",
    "\n",
    "\n",
    "\n",
    "print('INFO: Currently training using the {} NETWORK with regression = {} and classification = {}'.format(NETWORK,REG,CLA))\n",
    "if NETWORK == \"Vanilla\":\n",
    "    CNN = dm.build_cnn(X_res, Y_res, 3, regress=True)\n",
    "elif NETWORK == \"ALEXNET\":\n",
    "    CNN = alexnet(X_res, Y_res, 3,classes=2,regress = REG,multi_label=True,classification=CLA,option=None)\n",
    "elif NETWORK == \"VGG\":\n",
    "    CNN = cnn_vgg(X_res, Y_res, 3,classes=2,regress = REG,multi_label=True,classification=CLA,option=None)\n",
    "elif NETWORK == \"RESNET50\":\n",
    "    CNN = Resnet50(X_res, Y_res, 3,classes=2,regress = REG,multi_label=True,classification=CLA,option=None)\n",
    "    # CNN = ocn.ResNet50(X_res, Y_res, 3,classes=2,regress = REG,multi_label=True,classification=CLA,option=None)\n",
    "elif NETWORK == \"TL\":    \n",
    "    CNN = TRANSFERLEARNING(X_res, Y_res, 3,classes=2,regress = REG,multi_label=True,classification=CLA,option=None,transfer_model=transfer_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(init_lr, decay=init_lr/epochs)\n",
    "\n",
    "\n",
    "if REG == True and CLA ==False:\n",
    "\n",
    "    # When used for regression\n",
    "    CNN.compile(loss='mean_squared_error',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "\n",
    "    CNN_history = CNN.fit(x=trainImagesX, y=train_labels,\n",
    "                      validation_split = 0.15,epochs=epochs, batch_size=batch_size,callbacks=[early_stop])\n",
    "\n",
    "if REG == False and CLA ==True:\n",
    "    #When used for classification\n",
    "    CNN.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "    CNN_history = CNN.fit(x=X_train, y=Y_train,\n",
    "                      validation_split = 0.15,epochs=epochs, batch_size=batch_size,callbacks=[early_stop])\n",
    "\n",
    "\n",
    "if REG == True and CLA ==True:\n",
    "    # When used for classification and regression\n",
    "    CNN.compile(loss=['mean_squared_error','binary_crossentropy'],optimizer=optimizer,metrics=['mean_squared_error', 'accuracy'])\n",
    "\n",
    "    CNN_history = CNN.fit(x=X_train, y=[train_labels,Y_train],\n",
    "                      validation_split = 0.15,epochs=epochs, batch_size=batch_size,callbacks=[early_stop])\n",
    "\n",
    "# print('Memory (After Training): {}Mb'.format(mem_profile.memory_usage()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the network and the loss history for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "executionInfo": {
     "elapsed": 1558,
     "status": "error",
     "timestamp": 1657728089375,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "8m9VZnHrU-6C",
    "outputId": "bdf411c1-34e6-406f-f64c-d7f35314c812"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/saved_model/TL_ResNet50_64_modelC/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/saved_model/TL_ResNet50_64_modelC/assets\n"
     ]
    }
   ],
   "source": [
    "hist_df = pd.DataFrame(CNN_history.history)  ## converting to dataframe for future usels\n",
    "## Saving the history and the model\n",
    "if NETWORK == \"TL\":\n",
    "    if REG == True and CLA == True:\n",
    "        CNN.save(path+'saved_model/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelRC')\n",
    "        hist_df.to_csv(path+'data_folder/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelRC'+'_history.csv')\n",
    "    elif REG == True and CLA == False:\n",
    "        CNN.save(path+'saved_model/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelR')\n",
    "        hist_df.to_csv(path+'data_folder/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelR'+'_history.csv')    \n",
    "    elif CLA == True and REG == False:\n",
    "        CNN.save(path+'saved_model/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelC')        \n",
    "        hist_df.to_csv(path+'data_folder/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelC'+'_history.csv')\n",
    "    \n",
    "else:    \n",
    "\n",
    "    if REG == True and CLA == True:\n",
    "        CNN.save(path+'saved_model/'+NETWORK+'_'+str(X_res)+'_modelRC')\n",
    "        hist_df.to_csv(path+'data_folder/'+NETWORK+'_'+str(X_res)+'_modelRC'+'_history.csv')\n",
    "    elif REG == True and CLA == False:\n",
    "        CNN.save(path+'saved_model/'+NETWORK+'_'+str(X_res)+'_modelR')\n",
    "        hist_df.to_csv(path+'data_folder/'+NETWORK+'_'+str(X_res)+'_modelR'+'_history.csv')    \n",
    "    elif CLA == True and REG == False:\n",
    "        CNN.save(path+'saved_model/'+NETWORK+'_'+str(X_res)+'_modelC')\n",
    "        hist_df.to_csv(path+'data_folder/'+NETWORK+'_'+str(X_res)+'_modelC'+'_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 193,
     "status": "aborted",
     "timestamp": 1657728089061,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "HBFZbKbLU-6D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:The Trained model TL_ResNet50 at res 64 is loaded \n"
     ]
    }
   ],
   "source": [
    "# Loading the model\n",
    "\n",
    "if NETWORK == \"TL\":\n",
    "    if REG == True and CLA == True:\n",
    "        CNN = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelRC')\n",
    "    elif REG == True:\n",
    "        CNN = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelR')\n",
    "    elif CLA == True:\n",
    "        CNN = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelC')\n",
    "else:\n",
    "    if REG == True and CLA == True:\n",
    "        CNN = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'_'+str(X_res)+'_modelRC')\n",
    "    elif REG == True:\n",
    "        CNN = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'_'+str(X_res)+'_modelR')\n",
    "    elif CLA == True:\n",
    "        CNN = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'_'+str(X_res)+'_modelC')\n",
    "\n",
    "print(\"INFO:The Trained model {} at res {} is loaded \".format(NETWORK+'_'+str( transfer_model.__name__),str(X_res)))\n",
    "\n",
    "# CNN = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'_'+str(X_res)+'_modelRC')\n",
    "#Check its architecture\n",
    "# CNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSrA8naXyvf2"
   },
   "source": [
    "### Model Predictions and Results for the regression and Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1129,
     "status": "ok",
     "timestamp": 1657735078291,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "emN-GUosypdf",
    "outputId": "b2f7e097-fbf8-4ec2-ff34-b81e342437e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 3s 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.68380404, 0.4653286 ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index = 41 # 550  #550 ##210\n",
    "pred_CNN = CNN.predict(testImagesX)\n",
    "  \n",
    "# np.shape(pred_CNN)\n",
    "# print(pred_CNN[1][test_index])\n",
    "# print(pred_CNN[0][test_index])\n",
    "\n",
    "pred_CNN[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1657735080155,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "F82PAY5_eAIy",
    "outputId": "9a81cf8b-90a6-482b-8bf5-dd650a0f33dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted prbability of the presence of planets are [0.68380404 0.4653286 ] and \n",
      "The True values are \n",
      "PM2    1\n",
      "PM3    1\n",
      "Name: 88895, dtype: int64 \n",
      "The predicted Values are [0.68380404 0.4653286 ] and \n",
      "The True values are \n",
      "Planet_Mass1    363.333333\n",
      "Planet_Mass2    233.000000\n",
      "Planet_Mass3    846.666667\n",
      "Name: 88895, dtype: float64 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(testImagesX[test_index])\n",
    "test_labels.iloc[test_index]\n",
    "if REG == True:\n",
    "    print(\"The predicted Values are {} and \\nThe True values are \\n{} \".format(pred_CNN[test_index],test_labels.iloc[test_index]))\n",
    "elif CLA == True:\n",
    "    print(\"The predicted prbability of the presence of planets are {} and \\nThe True values are \\n{} \".format(pred_CNN[test_index],Y_test.iloc[test_index]))\n",
    "    print(\"The predicted Values are {} and \\nThe True values are \\n{} \".format(pred_CNN[test_index],test_labels.iloc[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1657731316030,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "G9IBNHMW1U8z",
    "outputId": "fc434448-df78-4d7b-d70a-c4f1f9dcbb05"
   },
   "outputs": [],
   "source": [
    "# CNN_history.history\n",
    "# Y_test.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "CNN_history.history ##val_fc_mean_squared_error val_fc_accuracy' val_dense_3_mean_squared_error val_dense_3_accuracy\n",
    "\n",
    "###fc_mean_squared_error fc_accuracy dense_3_mean_squared_error dense_3_accuracy\n",
    "hist =pd.DataFrame(CNN_history.history)\n",
    "hist['epoch'] = hist.index = hist.index\n",
    "hist\n",
    "plot_name = NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)\n",
    "if  CLA == True and REG== False:\n",
    "    cla_acc = CNN_history.history['accuracy']\n",
    "    val_cla_acc = CNN_history.history['val_accuracy']\n",
    "    cla_loss =CNN_history.history['loss']\n",
    "    val_cla_loss = CNN_history.history['val_loss']\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    # plt.subplot(2, 2, 1)\n",
    "    plt.plot(hist['epoch'], cla_acc, label='cla-Training Accuracy')\n",
    "    plt.plot(hist['epoch'], val_cla_acc, label='cla-Validation Accuracy')\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Classification Training and Validation Accuracy')\n",
    "    plt.savefig('figures/'+plot_name+'_c.png')\n",
    "    plt.show()\n",
    "    \n",
    "if REG == True and CLA == False:\n",
    "    ## For Regression\n",
    "    reg_mse = CNN_history.history['mean_squared_error']\n",
    "    val_reg_mse =CNN_history.history['val_mean_squared_error']\n",
    "    reg_loss =CNN_history.history['loss']\n",
    "    val_reg_loss = CNN_history.history['val_loss']\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    # plt.subplot(2, 2, 1)\n",
    "    plt.ylim(0,50000)\n",
    "    plt.plot(hist['epoch'], reg_mse, label='reg-Training MSE')\n",
    "    plt.plot(hist['epoch'], val_reg_mse, label='reg-Validation MSE')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Regression Training and Validation MSE')\n",
    "    plt.savefig('figures/'+plot_name+'_r.png')\n",
    "\n",
    "if REG == True and CLA == True:\n",
    "    ## For classification\n",
    "    cla_acc = CNN_history.history['cla_accuracy']\n",
    "    val_cla_acc = CNN_history.history['val_cla_accuracy']\n",
    "    cla_loss =CNN_history.history['cla_loss']\n",
    "    val_cla_loss = CNN_history.history['val_cla_loss']\n",
    "\n",
    "    ## For Regression\n",
    "    reg_mse = CNN_history.history['reg_mean_squared_error']\n",
    "    val_reg_mse =CNN_history.history['val_reg_mean_squared_error']\n",
    "    reg_loss =CNN_history.history['reg_loss']\n",
    "    val_reg_loss = CNN_history.history['val_reg_loss']\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(hist['epoch'], cla_acc, label='cla-Training Accuracy')\n",
    "    plt.plot(hist['epoch'], val_cla_acc, label='cla-Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Classification Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(hist['epoch'], reg_mse, label='reg-Training MSE')\n",
    "    plt.plot(hist['epoch'], val_reg_mse, label='reg-Validation MSE')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Regression Training and Validation MSE')\n",
    "    plt.savefig('figures/'+plot_name+'_cr.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DPNNet-RT_V3.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ML_env_2",
   "language": "python",
   "name": "ml_env_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
