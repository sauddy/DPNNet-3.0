{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sauddy/DPCNet/blob/main/DPCNet_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Av3XMdEU-5_"
   },
   "source": [
    "## DPNNet-RT -- 22 October 2022 (Colab Compatible)\n",
    "\n",
    "In this notebook we are using the data from the entire 700 simulations:\n",
    "\n",
    "This notebook is created to build a ML model that can Classify the number of hidden planets and Predict the \n",
    "corresponding planet mass for each of the planet from the protoplanetary disk images directly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-5y_R0ILrPP"
   },
   "source": [
    "#####    Please note this version of the code is compatible with Google colab \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37418,
     "status": "ok",
     "timestamp": 1657722963014,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "QG8TXKEFVeok",
    "outputId": "f8ebb99b-6a55-4997-cbf1-176ef13b2b11"
   },
   "outputs": [],
   "source": [
    "####mount the drive if running from Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3CDNlzfU-6A"
   },
   "source": [
    "### IDEA behind this notebook : \n",
    "#### Author : Sayantan \n",
    "#### Created : 3 Feb 2022\n",
    "#### This notebook is adopted from the DPNNet-2.0\n",
    "\n",
    "##### Updated1: 24 Feb 2022 to include multi-outputs\n",
    "##### Updated2: 22 October 2022 to include the all the date from the 700 FARGO3D calculations\n",
    "\n",
    "This notebook is developed to train the Model with RT images:\n",
    "We want to perform the following set of tasks\n",
    "\n",
    " P.S.This is a modular notebook that does the following:\n",
    " 1. Import all the customized Modules from Modules_DPNNet \n",
    " 2. For data processing we use RTdata_processing.py script \n",
    " 3. A functional module to call the different networks independently. (deep_models.py, other_cnn.py)\n",
    " 4. On October 2022, we are updating this notebook. We can now acess the complete data but shall choose randomly from the images\n",
    " 5. Still only considering the axysymmetric images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7338,
     "status": "ok",
     "timestamp": 1657722970345,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "hyZWiknOU-6A",
    "outputId": "4d843a4b-c0ca-44d2-9a54-ca21f2a1b8e5"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "\n",
    "## Modules to check the performance of the code\n",
    "from time import process_time \n",
    "# !pip install memory_profiler ## When running from Google Colab\n",
    "# import memory_profiler as mem_profile\n",
    "# print('Memory (Before): {}Mb'.format(mem_profile.memory_usage()))\n",
    "\n",
    "\n",
    "## Importing the necessary TesnorFLow modules modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# from tensorflow.python.compiler.mlcompute import mlcompute\n",
    "# mlcompute.set_mlc_device(device_name='gpu')\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import r2_score ## form calcualting the r2 score\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow import keras as k\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1657722970656,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "bx9nBvvHLrPR",
    "outputId": "1cbea282-af21-4c66-ee39-57e1a0aa81e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2412,
     "status": "ok",
     "timestamp": 1657722973065,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "h9468NQc2-Cq",
    "outputId": "287efd1b-8f8b-4f68-c362-cafedf843250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are currently using the Modules_DPCNet-RT\n",
      "Creation of the directory data_folder failed/ not needed as it already exit\n",
      "Creation of the directory figures failed/ not needed as it already exit\n",
      "Creation of the directory saved_model failed/ not needed as it already exit\n",
      "[INFO] Modules imported\n"
     ]
    }
   ],
   "source": [
    "############ Please provide the same path to the code directory if using Colab################\n",
    "\n",
    "# Path_gdrive= '/content/drive/MyDrive/DPNNet-RT/' ## Comment out this line if using local computer\n",
    "\n",
    "## Importing the Modules from Modules_DPNNet\n",
    "import sys\n",
    "try: ## tries to find the modules in the local directory first\n",
    "  current_directory = os.getcwd()\n",
    "  path = current_directory + '/' # For local computer \n",
    "#   path = '' # For local computer  \n",
    "  sys.path.append(path+'MODULES_DPNNeT')\n",
    "  import data_processing_RT as dp\n",
    "  import deep_models as dm\n",
    "  import other_cnns as ocn\n",
    "\n",
    "########### Folders to save the processed data, files and figures when using Local computer ##############\n",
    "  output_folder_list = ['data_folder','figures','saved_model']\n",
    "  for file in output_folder_list:\n",
    "    try:\n",
    "        os.makedirs(file)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed/ not needed as it already exit\" % file)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s\" % file)\n",
    "  \n",
    "except ModuleNotFoundError:\n",
    "  \n",
    "  # #For Colab use:\n",
    "  # #Point to the path containing the modules in the above section\n",
    "  #(data folder are a directory above the directory containing the notebook)\n",
    "  try:\n",
    "    path = Path_gdrive\n",
    "    print(path)\n",
    "    sys.path.append(path+'MODULES_DPNNeT')\n",
    "    import data_processing_RT as dp\n",
    "    import deep_models as dm\n",
    "    import other_cnns as ocn\n",
    "\n",
    "    ########### Folders to save the processed data, files and figures when using GDRIVE ##############\n",
    "    import os\n",
    "    os.chdir(path)\n",
    "    print(\"Creating the folders\")\n",
    "    !mkdir -p data_folder\n",
    "    !mkdir -p figures ## to save the figures\n",
    "    !mkdir -p figures_paper\n",
    "    !mkdir -p saved_model\n",
    "  except ModuleNotFoundError:\n",
    "    print(\"The path to the modules is incorrect-- Provide current path\")\n",
    "\n",
    "print(\"[INFO] Modules imported\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldbzXokMU-6B"
   },
   "source": [
    "## Creating a csv with simulations params and path to each RT images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1657722973614,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "LGF9QKBiLrPT",
    "outputId": "e2f86397-dc98-4d4d-e32e-d39865db6e3a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Planet_Mass1</th>\n",
       "      <th>Planet_Mass2</th>\n",
       "      <th>Planet_Mass3</th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Stokes</th>\n",
       "      <th>Aspect_Ratio</th>\n",
       "      <th>SigmaSlope</th>\n",
       "      <th>Flaring_Index</th>\n",
       "      <th>Rp1</th>\n",
       "      <th>Rp2</th>\n",
       "      <th>Rp3</th>\n",
       "      <th>a_grain_mic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.19</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002280</td>\n",
       "      <td>0.00154</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>0.0561</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.943</td>\n",
       "      <td>2.16</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.00262</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.160</td>\n",
       "      <td>1.84</td>\n",
       "      <td>3.15</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>0.0496</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.99</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.00187</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.863</td>\n",
       "      <td>2.08</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.0454</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.29</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>0.002530</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.0882</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.851</td>\n",
       "      <td>1.99</td>\n",
       "      <td>3.10</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.130</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.35</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.0711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.966</td>\n",
       "      <td>2.02</td>\n",
       "      <td>3.32</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.0646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.010</td>\n",
       "      <td>1.87</td>\n",
       "      <td>3.25</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Planet_Mass1  Planet_Mass2  Planet_Mass3  Epsilon  Alpha    Stokes  \\\n",
       "0        0.000322       0.00204      0.001210    0.050  0.001  1.570000   \n",
       "1        0.002280       0.00154      0.000963    0.025  0.010  1.570000   \n",
       "2        0.002450       0.00262      0.002460    0.025  0.010  1.570000   \n",
       "3        0.000917       0.00195      0.001050    0.010  0.001  1.570000   \n",
       "4        0.001770       0.00187      0.002210    0.025  0.050  1.570000   \n",
       "..            ...           ...           ...      ...    ...       ...   \n",
       "695      0.002110       0.00000      0.000000    0.025  0.005  0.000157   \n",
       "696      0.002530       0.00000      0.000000    0.025  0.010  0.000157   \n",
       "697      0.000577       0.00000      0.000000    0.025  0.001  0.000157   \n",
       "698      0.001940       0.00000      0.000000    0.025  0.050  0.000157   \n",
       "699      0.002790       0.00000      0.000000    0.050  0.005  0.000157   \n",
       "\n",
       "     Aspect_Ratio  SigmaSlope  Flaring_Index    Rp1   Rp2   Rp3  a_grain_mic  \n",
       "0          0.0432         1.0          0.010  1.010  2.19  3.22       3000.0  \n",
       "1          0.0561         1.0          0.075  0.943  2.16  3.37       1000.0  \n",
       "2          0.0261         1.0          0.075  1.160  1.84  3.15      10000.0  \n",
       "3          0.0496         1.0          0.250  1.100  1.82  2.99      10000.0  \n",
       "4          0.0625         1.0          0.075  0.863  2.08  3.32       3000.0  \n",
       "..            ...         ...            ...    ...   ...   ...          ...  \n",
       "695        0.0454         1.0          0.010  1.100  1.94  3.29        100.0  \n",
       "696        0.0882         1.0          0.075  0.851  1.99  3.10        100.0  \n",
       "697        0.0732         1.0          0.250  1.130  2.00  3.35        100.0  \n",
       "698        0.0711         1.0          0.075  0.966  2.02  3.32        100.0  \n",
       "699        0.0646         1.0          0.250  1.010  1.87  3.25        100.0  \n",
       "\n",
       "[700 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Taking a look at the paramter file \n",
    "parameter_df = dp.load_parameter_csv(path)\n",
    "parameter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Move the images from the original folder to the code directory \n",
    "\n",
    "Only needed initially for moving the images to the working directory--\n",
    "Ones the data is moved there is no need for this code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1657722973615,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "GJh7NLv7U-6B"
   },
   "outputs": [],
   "source": [
    "############ to move the images from the original folder to the code directory ###################\n",
    "## Path to the RT image folder\n",
    "# RT_Folder_Path = \"/Users/sauddy/OneDrive - Iowa State University/images/examples/\"\n",
    "\n",
    "# RT_Folder_Path = \"/home/sauddy3/scratch/DPNNet-RT/radmc3d-2.0/examples/\"\n",
    "\n",
    "\n",
    "# list_RT_path = glob.glob(RT_Folder_Path+ 'RT_A*') ## make a list of all the RT folder where each folder is for each sim\n",
    "# # print(list_RT_path)\n",
    "\n",
    "# list_sorted_RT_path  = sorted(list_RT_path, key=lambda x: int(x.split('/')[7].split('_')[2])) ## sorting the images\n",
    "# print(list_sorted_RT_path)\n",
    "\n",
    "# ############### Moved the images to the code directory for convenience ######\n",
    "# import shutil\n",
    "# try:\n",
    "#     os.mkdir('image_directory_complete')\n",
    "# except OSError:\n",
    "#      print (\"Creation of the directory %s failed/ not needed as it already exit\" % file)\n",
    "# NEW_IMAGE_PATH = 'image_directory_complete'\n",
    "# os.chdir(NEW_IMAGE_PATH)\n",
    "# for index in range(len(list_sorted_RT_path)):\n",
    "#     try:\n",
    "#         os.mkdir('RT_A_'+ str(index+1))\n",
    "#     except OSError:\n",
    "# #          print (\"Creation of the directory %s failed/ not needed as it already exit\" % file)\n",
    "#          pass\n",
    "    \n",
    "#     destination_path = 'RT_A_'+ str(index+1)\n",
    "#     for img in glob.glob(list_sorted_RT_path[index] + \"/images/snu/\"+ \"image_\"+\"*.png\"):\n",
    "#         shutil.copy(img, destination_path)\n",
    "\n",
    "# os.chdir('../')\n",
    "\n",
    "############## END ##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Address to the data folder \n",
    "\n",
    "Genrating the .csv file with the image address to load them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "executionInfo": {
     "elapsed": 31437,
     "status": "ok",
     "timestamp": 1657723005044,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "hB64_GiIlyWP",
    "outputId": "88ce034a-ec64-4ace-bba2-db4ea2d4da46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Importing path for all the RT images\n",
      "[INFO]: Contatinating the paths of all the RT images is now complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Planet_Mass1</th>\n",
       "      <th>Planet_Mass2</th>\n",
       "      <th>Planet_Mass3</th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Stokes</th>\n",
       "      <th>Aspect_Ratio</th>\n",
       "      <th>SigmaSlope</th>\n",
       "      <th>Flaring_Index</th>\n",
       "      <th>Rp1</th>\n",
       "      <th>Rp2</th>\n",
       "      <th>Rp3</th>\n",
       "      <th>a_grain_mic</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77568</th>\n",
       "      <td>22.166667</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.851</td>\n",
       "      <td>1.82</td>\n",
       "      <td>2.91</td>\n",
       "      <td>300.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78688</th>\n",
       "      <td>363.333333</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.909</td>\n",
       "      <td>2.16</td>\n",
       "      <td>3.03</td>\n",
       "      <td>300.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70741</th>\n",
       "      <td>730.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>376.666667</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.140</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.96</td>\n",
       "      <td>300.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>703.333333</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>516.666667</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.150</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.06</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41656</th>\n",
       "      <td>930.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.931</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69041</th>\n",
       "      <td>420.000000</td>\n",
       "      <td>333.333333</td>\n",
       "      <td>460.000000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.0946</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.150</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.37</td>\n",
       "      <td>100.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38479</th>\n",
       "      <td>192.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.840</td>\n",
       "      <td>1.81</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92020</th>\n",
       "      <td>503.333333</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>986.666667</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.97</td>\n",
       "      <td>3.05</td>\n",
       "      <td>100.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5466</th>\n",
       "      <td>703.333333</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>516.666667</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.150</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.06</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21950</th>\n",
       "      <td>986.666667</td>\n",
       "      <td>566.666667</td>\n",
       "      <td>876.666667</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.840</td>\n",
       "      <td>2.14</td>\n",
       "      <td>3.17</td>\n",
       "      <td>300.0</td>\n",
       "      <td>/scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Planet_Mass1  Planet_Mass2  Planet_Mass3  Epsilon   Alpha    Stokes  \\\n",
       "77568     22.166667     60.000000      0.000000    0.050  0.0010  0.001570   \n",
       "78688    363.333333     60.000000    240.000000    0.010  0.0010  0.001570   \n",
       "70741    730.000000     60.000000    376.666667    0.025  0.0500  0.001570   \n",
       "5447     703.333333     60.000000    516.666667    0.010  0.0050  1.570000   \n",
       "41656    930.000000      0.000000      0.000000    0.050  0.0001  0.015700   \n",
       "...             ...           ...           ...      ...     ...       ...   \n",
       "69041    420.000000    333.333333    460.000000    0.025  0.0001  0.001570   \n",
       "38479    192.333333      0.000000      0.000000    0.025  0.0100  0.015700   \n",
       "92020    503.333333     60.000000    986.666667    0.025  0.0500  0.000157   \n",
       "5466     703.333333     60.000000    516.666667    0.010  0.0050  1.570000   \n",
       "21950    986.666667    566.666667    876.666667    0.025  0.0100  0.015700   \n",
       "\n",
       "       Aspect_Ratio  SigmaSlope  Flaring_Index    Rp1   Rp2   Rp3  \\\n",
       "77568        0.0732         1.0          0.075  0.851  1.82  2.91   \n",
       "78688        0.0818         1.0          0.010  0.909  2.16  3.03   \n",
       "70741        0.0411         1.0          0.075  1.140  2.15  2.96   \n",
       "5447         0.0732         1.0          0.250  1.150  2.05  3.06   \n",
       "41656        0.0989         1.0          0.010  0.931  2.05  3.46   \n",
       "...             ...         ...            ...    ...   ...   ...   \n",
       "69041        0.0946         1.0          0.075  1.150  2.00  3.37   \n",
       "38479        0.0304         1.0          0.010  0.840  1.81  3.42   \n",
       "92020        0.0689         1.0          0.010  1.020  1.97  3.05   \n",
       "5466         0.0732         1.0          0.250  1.150  2.05  3.06   \n",
       "21950        0.0282         1.0          0.075  0.840  2.14  3.17   \n",
       "\n",
       "       a_grain_mic                                         image_path  \n",
       "77568        300.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "78688        300.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "70741        300.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "5447        1000.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "41656       3000.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "...            ...                                                ...  \n",
       "69041        100.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "38479       1000.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "92020        100.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "5466        1000.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "21950        300.0  /scratch/sauddy3/DPNNet-RT/DPNNet-RT-ML-Code/i...  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############# Address to the data folder ###################\n",
    "\n",
    "# ## updating the image paths once the transfer is done\n",
    "list_RT_path = glob.glob(path+ 'image_directory_complete/'+ 'RT_A*') ## make a list of all the RT folder where each folder is for each sim\n",
    "\n",
    "## When using the local M1\n",
    "# list_sorted_RT_path  = sorted(list_RT_path, key=lambda x: int(x.split('/')[1].split('_')[2])) ## sorting the images\n",
    "\n",
    "\n",
    "# ## For google colab this needs to be updated\n",
    "list_sorted_RT_path  = sorted(list_RT_path, key=lambda x: int(x.split('/')[6].split('_')[2])) ## sorting the images\n",
    "\n",
    "\n",
    "############## Will be removed once all the images are ready and tested ####\n",
    "# print(list_sorted_RT_path)\n",
    "# df_images_folder_complete=[]\n",
    "# for index in range(len(list_sorted_RT_path)):\n",
    "\n",
    "#     path_image = list_sorted_RT_path[index] ## path to each RT folder\n",
    "\n",
    "#     ## for paths from the RT sim fodlers directly\n",
    "#     list_image_path = glob.glob(path_image + \"/images/snu/\"+ \"image_\"+\"*.png\") ## list of the path to each image in the RT folder\n",
    "#     if len(list_image_path) ==0 :## for updates image paths\n",
    "#         # print(\"Reading images from the updated folder\")\n",
    "#         list_image_path = glob.glob(path_image+'/'+\"*.png\") ## list of the path to each image in the RT folder\n",
    "#         print(\"index ={}, numberofimges ={}\".format(index,len(list_image_path)))\n",
    "#     df_images_folder =pd.DataFrame(list_image_path,columns=[\"image_path\"])\n",
    "#     df_images_folder_complete.append(df_images_folder)\n",
    "    \n",
    "    \n",
    "# df_images_folder_complete = pd.concat(df_images_folder_complete, ignore_index=True, axis=0)\n",
    "# df_images_folder_complete\n",
    "# The idea is to generate a dataframe with the parameters and the path to the images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_complete = dp.create_complete_data_csv(list_sorted_RT_path,path)\n",
    "# data_complete\n",
    "## Removing the nan if any\n",
    "data_complete.isna().sum()  # summing the number of na\n",
    "data_complete= data_complete.dropna()\n",
    "data_complete\n",
    "\n",
    "# data_complete['Planet_Count'] = (data_complete.loc[:, ['Planet_Mass1', 'Planet_Mass2', 'Planet_Mass3']] != 0).sum(axis=1)\n",
    "data_complete = shuffle(data_complete,random_state=42)\n",
    "data_complete= data_complete[:10000] ## Select less numbers of images for quick results\n",
    "data_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDxiZTS9U-6B"
   },
   "source": [
    "## Preparing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3070060,
     "status": "ok",
     "timestamp": 1657726075097,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "agTa7GkoU-6B",
    "outputId": "61a0667a-2edb-4fda-bb4b-247932fbea5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] preparing the normalized data training/testing split...\n",
      "Droping the irrelevant columns \n",
      "INFO: considering multiple planets as output\n",
      "[INFO] Done...\n",
      "Found 7225 validated image filenames.\n",
      "Found 1275 validated image filenames.\n",
      "Found 1500 validated image filenames.\n",
      "RAM memory % used: 3.91\n"
     ]
    }
   ],
   "source": [
    "## partition the data csv file into training and testing splits using 85% of\n",
    "## the data for training and the remaining 15% for testing\n",
    "split = train_test_split(data_complete, test_size=0.15, random_state=42)\n",
    "(train, test) = split\n",
    "\n",
    "## Save the train and the test data for future use as well.\n",
    "test.to_csv(path+'data_folder/test_dataset.csv')\n",
    "train.to_csv(path+'data_folder/train_dataset.csv')\n",
    "\n",
    "#### Desired Image resoltuion #####\n",
    "X_res = Y_res = 256\n",
    "\n",
    "## Generate the Normalized data\n",
    "normed_train_data, normed_test_data, train_labels, test_labels = dp.process_the_disk_attributes(train, test, path,multi_label=True)\n",
    "\n",
    "\n",
    "# def cropping_function(image):\n",
    "    \n",
    "#     # dimensions for cropping the image\n",
    "#     top = 54\n",
    "#     left = 102\n",
    "#     bottom = 430\n",
    "#     right = 480  \n",
    "#     image = cv2.imread(imagePath)\n",
    "#     cropped_image = image[top:bottom, left:right]\n",
    "#     return cropped_image\n",
    "\n",
    "def custom_augmentation(np_tensor):\n",
    "    \n",
    "    def crop(np_tensor):\n",
    "        \n",
    "        offset_height =50\n",
    "        offset_width = 50\n",
    "        target_height = 512\n",
    "        target_width = 512 \n",
    "        cropped = tf.image.crop_to_bounding_box(np_tensor, offset_height, offset_width, target_height, target_width)\n",
    "        \n",
    "        return tf.image.resize(cropped, np_tensor.shape[:2])\n",
    " \n",
    "    def random_crop(np_tensor):\n",
    "\n",
    "        #cropped height between 70% to 130% of an original height\n",
    "        new_height = int(np.random.uniform(0.1, 1.10) * np_tensor.shape[0])\n",
    "\n",
    "        #cropped width between 70% to 130% of an original width\n",
    "        new_width = int(np.random.uniform(0.1, 1.30) * np_tensor.shape[1])\n",
    "\n",
    "        # resize to new height and width\n",
    "        cropped = tf.image.resize_with_crop_or_pad(np_tensor, new_height, new_width)\n",
    "\n",
    "        return tf.image.resize(cropped, np_tensor.shape[:2])\n",
    "\n",
    "    augmnted_tensor = crop(np_tensor)\n",
    "    return np.array(augmnted_tensor)\n",
    "\n",
    "# datagen=ImageDataGenerator(preprocessing_function=custom_augmentation,samplewise_center=True, samplewise_std_normalization=True,rescale=1./255.,validation_split=0.15)\n",
    "\n",
    "datagen=ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True,rescale=1./255.,validation_split=0.15)\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "dataframe=train,\n",
    "directory=None,\n",
    "x_col=\"image_path\",\n",
    "y_col=[\"Planet_Mass1\",'Planet_Mass2','Planet_Mass3'],\n",
    "subset=\"training\",\n",
    "batch_size=32,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\",\n",
    "target_size=(X_res,Y_res))\n",
    "\n",
    "\n",
    "validation_generator=datagen.flow_from_dataframe(\n",
    "dataframe=train,\n",
    "directory=None,\n",
    "x_col=\"image_path\",\n",
    "y_col=[\"Planet_Mass1\",'Planet_Mass2','Planet_Mass3'],\n",
    "subset=\"validation\",\n",
    "batch_size=32,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"raw\",\n",
    "target_size=(X_res,Y_res))\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_generator=datagen.flow_from_dataframe(\n",
    "dataframe=test,\n",
    "directory=None,\n",
    "x_col=\"image_path\",\n",
    "y_col=[\"Planet_Mass1\",'Planet_Mass2','Planet_Mass3'],\n",
    "batch_size=32,\n",
    "seed=42,\n",
    "shuffle=False,\n",
    "class_mode=None,\n",
    "target_size=(X_res,Y_res))\n",
    "\n",
    " \n",
    "# Getting all memory using os.popen()\n",
    "total_memory, used_memory, free_memory = map(\n",
    "    int, os.popen('free -t -m').readlines()[-1].split()[1:])\n",
    " \n",
    "# Memory usage\n",
    "print(\"RAM memory % used:\", round((used_memory/total_memory) * 100, 2))\n",
    "## Generate the training and the test images \n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# trainImagesX = dp.load_disk_images(train, X_res, Y_res, Type = \"Train\")\n",
    "# testImagesX = dp.load_disk_images(test, X_res, Y_res, Type = \"Test\")\n",
    "\n",
    "# Validation_split = 0.15 # 15 percent of the training data is used for validation\n",
    "# # print('Memory (After Loading): {}Mb'.format(mem_profile.memory_usage()))\n",
    "# print('There are {} Train, {} Validation and {} Test images'.format(int((1-Validation_split)*len(normed_train_data)),int(Validation_split*len(normed_train_data)),len(normed_test_data)))## check the numbers in each category\n",
    "# end = time.time()\n",
    "# print(\"Total time elapsed =\", end - start)\n",
    "\n",
    "# # Getting all memory using os.popen()\n",
    "# total_memory, used_memory, free_memory = map(\n",
    "#     int, os.popen('free -t -m').readlines()[-1].split()[1:])\n",
    " \n",
    "# # Memory usage\n",
    "# print(\"RAM memory % used:\", round((used_memory/total_memory) * 100, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1657726075101,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "s-RddkHKLrPV"
   },
   "outputs": [],
   "source": [
    "# for _ in range(5):\n",
    "#     img, label = train_generator.next()\n",
    "#     print(img.shape)   #  (1,256,256,3)\n",
    "#     plt.imshow(img[0])\n",
    "#     plt.show()\n",
    "# filenames =test_generator.filenames\n",
    "# # filenames[1]\n",
    "# # train_labels\n",
    "# # test\n",
    "# # normed_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Checking one image for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1657726075102,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "LPN1iwW6LrPV"
   },
   "outputs": [],
   "source": [
    "# test_ = filenames[10]\n",
    "# print(test_)\n",
    "# # # dimensions for cropping the image\n",
    "# # top = 54\n",
    "# # left = 102\n",
    "# # bottom = 430\n",
    "# # right = 480\n",
    "# # X_res = Y_res = 512\n",
    "# image = cv2.imread(test_)  \n",
    "# # crop_image = image[top:bottom, left:right]\n",
    "# # crop_image = cv2.resize(crop_image, (X_res, Y_res)) \n",
    "# # crop_image = k.preprocessing.image.img_to_array(crop_image) ## changing to numpy array\n",
    "# # datagen = k.preprocessing.image.ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True,rescale= 1.0/255.0)\n",
    "# # crop_image = datagen.standardize(np.copy(crop_image))\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1657726075280,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "6jXw5UJ5LrPV",
    "outputId": "341e19e1-032e-4fde-8154-2f398f7cf92d"
   },
   "outputs": [],
   "source": [
    "# np.shape(trainImagesX[1])\n",
    "# plt.imshow(trainImagesX[5])\n",
    "# train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjjWc7sq9amp",
    "tags": []
   },
   "source": [
    "### Preparing the dataset for the classification problem--- \n",
    "\n",
    "##### We are using the one-hot encoding to label the one, two, and three planet system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1657731337855,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "MFng-6cgb1DC"
   },
   "outputs": [],
   "source": [
    "# X_train=trainImagesX\n",
    "# X_test=testImagesX\n",
    "\n",
    "# ## For classification we included binary 1 or 0 dependent on if the planet is present or not respectively\n",
    "# ## Since Planet 1 is always there we do it for planet 2 and 3 only\n",
    "\n",
    "# # train['PM1'] = np.where(train['Planet_Mass1']!= 0, 1,0)\n",
    "# train['PM2'] = np.where(train['Planet_Mass2']!= 0, 1,0)\n",
    "# train['PM3'] = np.where(train['Planet_Mass3']!= 0, 1,0)\n",
    "\n",
    "# # test['PM1'] = np.where(test['Planet_Mass1']!= 0, 1, 0)\n",
    "# test['PM2'] = np.where(test['Planet_Mass2']!= 0, 1, 0)\n",
    "# test['PM3'] = np.where(test['Planet_Mass3']!= 0, 1, 0)\n",
    "\n",
    "# Y_train=train[[\"PM2\",\"PM3\"]]\n",
    "# Y_test=test[[\"PM2\",\"PM3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1657731338403,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "0pDAjtJc-1M7",
    "outputId": "01e36eff-6c17-42da-9077-a648ea90ca66"
   },
   "outputs": [],
   "source": [
    "# Y_test\n",
    "# Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Different CNN models\n",
    "\n",
    "Will be later moved to the module folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transfer Learning Codes\n",
    "\n",
    "Will be later moved to the Module Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1 \n",
    "from tensorflow.keras.applications import EfficientNetB2, EfficientNetB3\n",
    "from tensorflow.keras.applications import EfficientNetB4, EfficientNetB5 \n",
    "from tensorflow.keras.applications import EfficientNetB6, EfficientNetB7\n",
    "from tensorflow.keras.applications import ResNet50,ResNet50V2\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "# modules added for the RESNET50\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.layers import ZeroPadding2D, AveragePooling2D, GlobalMaxPooling2D, Add\n",
    "\n",
    "\n",
    "def TRANSFERLEARNING(width,height,depth,classes=None,regress=False,multi_label=False,classification=False,option=None,transfer_model= None):\n",
    "\n",
    "    '''\n",
    "    This function introduces the use of Transfer learning to train the DPNNet Model\n",
    "    Here we use the pretrained weights from the Imagenet data set\n",
    "    \n",
    "    Please note this here update the final layers to perform regress or classification\n",
    "    or both regression and classification together\n",
    "    \n",
    "    Input:  1. Dimension of the image [width,height,depth]\n",
    "            2. Number of classsification classes if used for calssification\n",
    "            3. Regress on or off\n",
    "            4. Multilabel -- True for multi-label regression Default == False\n",
    "            5. Classification -- True or False \n",
    "            6. Option is the argument to use either softmax (default) or sigmoid for classification\n",
    "            7. Select the transfer Learning Model from Either 3 RESNET50 or Efficientnet0-7 models\n",
    "            \n",
    "            \n",
    "    Output : Model Build \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ## Input layers\n",
    "    X_input = layers.Input(shape=(X_res, Y_res, depth))\n",
    "    x = X_input\n",
    "    \n",
    "    if transfer_model == None:\n",
    "        transfer_model = EfficientNetB6\n",
    "        print(\"Please Select the Network that will be used-- By Default {} will be used\".format(str.lower(transfer_model.__name__)))\n",
    "    \n",
    "    #### Selecting the model along with the pretrained weights\n",
    "    mod_name = transfer_model.__name__\n",
    "    print(\"INFO: Keras Model used is {}\".format(mod_name))\n",
    "    print(\"Loading weights for {} since this notebook is not connected to internet\".format(str.lower(mod_name)))\n",
    "    \n",
    "    try:\n",
    "        model = transfer_model(include_top=False,input_tensor=x, weights='preloaded_weights/'+str.lower(mod_name)+'_notop.h5')\n",
    "    except ValueError:\n",
    "        model = transfer_model(include_top=False,input_tensor=x, weights='preloaded_weights/'+str.lower(mod_name)+'_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "    \n",
    "    \n",
    "    # Freeze the pretrained weights\n",
    "    model.trainable = True\n",
    "\n",
    "    # Rebuild top\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    top_dropout_rate = 0.2\n",
    "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "\n",
    "    XX = Flatten()(x)\n",
    "    XX = Dense(100, kernel_initializer='he_uniform', activation='relu')(XX) \n",
    "    # outputs = layers.Dense(classes, activation=\"softmax\", name=\"pred\")(x)\n",
    "\n",
    "\n",
    "    if classification==True:\n",
    "      # print(\"A new fully Connexted layer is added\")\n",
    "       ## Adding a Fully connected layer for classification\n",
    "      Xclas = XX\n",
    "      # Xclas = BatchNormalization(axis=1)(Xclas)   \n",
    "      top_dropout_rate = 0.30\n",
    "      # Xclas = layers.Dropout(top_dropout_rate, name=\"top_dropout1\")(Xclas)  \n",
    "      Xclas = Dense(50, kernel_initializer='he_uniform', activation='relu')(Xclas)\n",
    "      Xclas = BatchNormalization(axis=1)(Xclas)\n",
    "      # top_dropout_rate = 0.20\n",
    "      # Xclas = layers.Dropout(top_dropout_rate, name=\"top_dropout2\")(Xclas)\n",
    "      Xclas = Dense(20, kernel_initializer='he_uniform', activation='relu')(Xclas)\n",
    "      Xclas = BatchNormalization(axis=1)(Xclas)\n",
    "        \n",
    "      print(\"INFO:CNN is used for classification\")\n",
    "      if option ==1:\n",
    "          print(\"Additional INFO:Using Softmax for classification\")\n",
    "          Xclas = Dense(units=classes,name='cla',activation='softmax')(Xclas) \n",
    "      else:  \n",
    "          print(\"Additional INFO: Using Sigmoid activation for classification\")\n",
    "          Xclas = Dense(units=classes, name='cla',activation='sigmoid')(Xclas)\n",
    "          out_clas = Xclas \n",
    "\n",
    "\n",
    "    if regress == True:\n",
    "      print(\"INFO:CNN is used for regression\")\n",
    "      if multi_label==False:           \n",
    "        Xreg = Dense(1, activation='linear', name='reg', kernel_initializer = glorot_uniform(seed=0))(XX)\n",
    "      if multi_label==True: \n",
    "        ## 28 Feb 2022 added the multi-label output \n",
    "        print(\"INFO: Note Multiple Labels are optimised during regression\")\n",
    "        Xreg = Dense(3, activation='linear', name='reg', kernel_initializer = glorot_uniform(seed=0))(XX)\n",
    "        out_reg = Xreg\n",
    "    \n",
    "\n",
    "    ### Create model\n",
    "\n",
    "    if classification == True and regress == True:\n",
    "      print(\"INFO: Performing both regression and classification -- Model Training\")\n",
    "      modelNEWCNN = Model(inputs = X_input,outputs=[out_reg, out_clas])\n",
    "    elif regress == False:\n",
    "      X = out_clas\n",
    "      print(\"INFO: Classification Model is being trained\")\n",
    "      modelNEWCNN = Model(inputs = X_input, outputs = X)\n",
    "    elif classification is False:\n",
    "      X = out_reg\n",
    "      print(\"INFO: Regression Model is being trained\")\n",
    "      modelNEWCNN = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return modelNEWCNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYGYS3pgU-6C",
    "tags": []
   },
   "source": [
    "## Training the CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1657737657204,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "vkIRL4NmU-6C"
   },
   "outputs": [],
   "source": [
    "## Hyper-Parameter to define\n",
    "batch_size = 100 ## 20 was for regression ## the best was for 200 last run\n",
    "valid_batch_size = 100\n",
    "epochs=40 ## best was 100\n",
    "init_lr = 1e-5 # 1e-5 (works for regression)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "# early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.2, patience=20, verbose=1, mode='min',restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lW02M4WU-6C",
    "outputId": "3ccb437f-c01f-4f29-c467-8b68f18545a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Currently training using the RESNET50 NETWORK with regression = True and classification = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 13:00:49.690482: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-29 13:00:50.307093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11319 MB memory:  -> device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:83:00.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:CNN is used for regression\n",
      "INFO: Note Multiple Labels are optimised during regression\n",
      "INFO: Regression Model is being trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14516/1146064732.py:45: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  CNN_history = CNN.fit_generator(generator=train_generator,\n",
      "2022-10-29 13:01:05.066558: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n",
      "2022-10-29 13:01:05.527276: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-29 13:01:05.530059: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-29 13:01:05.530119: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-10-29 13:01:05.532608: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-29 13:01:05.532682: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - ETA: 0s - loss: 154585.9688 - mean_absolute_error: 285.9072 - mean_squared_error: 154585.9688"
     ]
    }
   ],
   "source": [
    "## Select the Network type\n",
    "\n",
    "# NETWORK = \"Vanilla\" ## Cannot be uses at the moment\n",
    "# NETWORK = \"ALEXNET\"\n",
    "# NETWORK = \"VGG\"\n",
    "NETWORK = \"RESNET50\"\n",
    "\n",
    "                                ### When using Trasnfer Learning ######################\n",
    "# NETWORK = \"TL\"\n",
    "# transfer_model=ResNet50\n",
    "# transfer_model=EfficientNetB3\n",
    "\n",
    "## Select the kind of Traning ## Both can be selected as well\n",
    "REG = True #True  ## When choosing regression\n",
    "CLA = False #False #True  ## When Choosing Clasiffication\n",
    "\n",
    "\n",
    "\n",
    "print('INFO: Currently training using the {} NETWORK with regression = {} and classification = {}'.format(NETWORK,REG,CLA))\n",
    "if NETWORK == \"Vanilla\":\n",
    "    CNN = dm.build_cnn(X_res, Y_res, 3, regress=True)\n",
    "elif NETWORK == \"ALEXNET\":\n",
    "    CNN = alexnet(X_res, Y_res, 3,classes=2,regress = REG,multi_label=True,classification=CLA,option=None)\n",
    "elif NETWORK == \"VGG\":\n",
    "    CNN = cnn_vgg(X_res, Y_res, 3,classes=2,regress = REG,multi_label=True,classification=CLA,option=None)\n",
    "elif NETWORK == \"RESNET50\":\n",
    "    # CNN = Resnet50(X_res, Y_res, 3,classes=2,regress = REG,multi_label=True,classification=CLA,option=None)\n",
    "    CNN = ocn.Resnet50(X_res, Y_res, 3,classes=2,regress = REG,multi_label=True,classification=CLA,option=None)\n",
    "elif NETWORK == \"TL\":    \n",
    "    CNN = TRANSFERLEARNING(X_res, Y_res, 3,classes=2,regress = REG,multi_label=True,classification=CLA,option=None,transfer_model=transfer_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(init_lr, decay=init_lr/epochs)\n",
    "\n",
    "\n",
    "if REG == True and CLA ==False:\n",
    "\n",
    "    # When used for regression\n",
    "    CNN.compile(loss='mean_squared_error',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "    \n",
    "    STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "    STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
    "    STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "    CNN_history = CNN.fit_generator(generator=train_generator,\n",
    "                        steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                        validation_data=validation_generator,\n",
    "                        validation_steps=STEP_SIZE_VALID,\n",
    "                        epochs=1)\n",
    "   \n",
    " \n",
    "    # Getting all memory using os.popen()\n",
    "    total_memory, used_memory, free_memory = map(\n",
    "    int, os.popen('free -t -m').readlines()[-1].split()[1:])\n",
    " \n",
    "# Memory usage\n",
    "print(\"RAM memory % used:\", round((used_memory/total_memory) * 100, 2))\n",
    "\n",
    "    # CNN_history = CNN.fit(x=trainImagesX, y=train_labels,\n",
    "    #                   validation_split = 0.15,epochs=epochs, batch_size=batch_size,callbacks=[early_stop])\n",
    "\n",
    "if REG == False and CLA ==True:\n",
    "    #When used for classification\n",
    "    CNN.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "    CNN_history = CNN.fit(x=X_train, y=Y_train,\n",
    "                      validation_split = 0.15,epochs=epochs, batch_size=batch_size,callbacks=[early_stop])\n",
    "\n",
    "\n",
    "if REG == True and CLA ==True:\n",
    "    # When used for classification and regression\n",
    "    CNN.compile(loss=['mean_squared_error','binary_crossentropy'],optimizer=optimizer,metrics=['mean_squared_error', 'accuracy'])\n",
    "\n",
    "    CNN_history = CNN.fit(x=X_train, y=[train_labels,Y_train],\n",
    "                      validation_split = 0.15,epochs=epochs, batch_size=batch_size,callbacks=[early_stop])\n",
    "\n",
    "# print('Memory (After Training): {}Mb'.format(mem_profile.memory_usage()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the network and the loss history for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "executionInfo": {
     "elapsed": 1558,
     "status": "error",
     "timestamp": 1657728089375,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "8m9VZnHrU-6C",
    "outputId": "bdf411c1-34e6-406f-f64c-d7f35314c812"
   },
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(CNN_history.history)  ## converting to dataframe for future usels\n",
    "## Saving the history and the model\n",
    "if NETWORK == \"TL\":\n",
    "    if REG == True and CLA == True:\n",
    "        CNN.save(path+'saved_model/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelRC')\n",
    "        hist_df.to_csv(path+'data_folder/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelRC'+'_history.csv')\n",
    "    elif REG == True and CLA == False:\n",
    "        CNN.save(path+'saved_model/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelR')\n",
    "        hist_df.to_csv(path+'data_folder/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelR'+'_history.csv')    \n",
    "    elif CLA == True and REG == False:\n",
    "        CNN.save(path+'saved_model/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelC')        \n",
    "        hist_df.to_csv(path+'data_folder/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelC'+'_history.csv')\n",
    "    \n",
    "else:    \n",
    "\n",
    "    if REG == True and CLA == True:\n",
    "        CNN.save(path+'saved_model/'+NETWORK+'_'+str(X_res)+'_modelRC')\n",
    "        hist_df.to_csv(path+'data_folder/'+NETWORK+'_'+str(X_res)+'_modelRC'+'_history.csv')\n",
    "    elif REG == True and CLA == False:\n",
    "        CNN.save(path+'saved_model/'+NETWORK+'_'+str(X_res)+'_modelR')\n",
    "        hist_df.to_csv(path+'data_folder/'+NETWORK+'_'+str(X_res)+'_modelR'+'_history.csv')    \n",
    "    elif CLA == True and REG == False:\n",
    "        CNN.save(path+'saved_model/'+NETWORK+'_'+str(X_res)+'_modelC')\n",
    "        hist_df.to_csv(path+'data_folder/'+NETWORK+'_'+str(X_res)+'_modelC'+'_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 193,
     "status": "aborted",
     "timestamp": 1657728089061,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "HBFZbKbLU-6D"
   },
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "\n",
    "if NETWORK == \"TL\":\n",
    "    if REG == True and CLA == True:\n",
    "        CNN = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelRC')\n",
    "    elif REG == True:\n",
    "        CNN = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelR')\n",
    "    elif CLA == True:\n",
    "        CNN = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)+'_modelC')\n",
    "else:\n",
    "    if REG == True and CLA == True:\n",
    "        CNN = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'_'+str(X_res)+'_modelRC')\n",
    "    elif REG == True:\n",
    "        CNN = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'_'+str(X_res)+'_modelR')\n",
    "    elif CLA == True:\n",
    "        CNN = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'_'+str(X_res)+'_modelC')\n",
    "\n",
    "print(\"INFO:The Trained model {} at res {} is loaded \".format(NETWORK+'_'+str( transfer_model.__name__),str(X_res)))\n",
    "\n",
    "# CNN = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'_'+str(X_res)+'_modelRC')\n",
    "#Check its architecture\n",
    "# CNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSrA8naXyvf2"
   },
   "source": [
    "### Model Predictions and Results for the regression and Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1129,
     "status": "ok",
     "timestamp": 1657735078291,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "emN-GUosypdf",
    "outputId": "b2f7e097-fbf8-4ec2-ff34-b81e342437e6"
   },
   "outputs": [],
   "source": [
    "# test_index = 41 # 550  #550 ##210\n",
    "# pred_CNN = CNN.predict(testImagesX)\n",
    "  \n",
    "# # np.shape(pred_CNN)\n",
    "# # print(pred_CNN[1][test_index])\n",
    "# # print(pred_CNN[0][test_index])\n",
    "\n",
    "# pred_CNN[test_index]\n",
    "\n",
    "\n",
    "test_generator.reset()\n",
    "pred=CNN.predict_generator(test_generator,\n",
    "steps=STEP_SIZE_TEST,\n",
    "verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1657735080155,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "F82PAY5_eAIy",
    "outputId": "9a81cf8b-90a6-482b-8bf5-dd650a0f33dd"
   },
   "outputs": [],
   "source": [
    "# plt.imshow(testImagesX[test_iclass_mode)\n",
    "test_index = 531\n",
    "if REG == True:\n",
    "    print(\"The predicted Values are {} and \\nThe True values are \\n{} \".format(pred[test_index],test_labels.iloc[test_index]))\n",
    "elif CLA == True:\n",
    "    print(\"The predicted prbability of the presence of planets are {} and \\nThe True values are \\n{} \".format(pred_CNN[test_index],Y_test.iloc[test_index]))\n",
    "    print(\"The predicted Values are {} and \\nThe True values are \\n{} \".format(pred_CNN[test_index],test_labels.iloc[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1657731316030,
     "user": {
      "displayName": "Sayantan Auddy",
      "userId": "04517631375485218215"
     },
     "user_tz": 240
    },
    "id": "G9IBNHMW1U8z",
    "outputId": "fc434448-df78-4d7b-d70a-c4f1f9dcbb05"
   },
   "outputs": [],
   "source": [
    "# CNN_history.history\n",
    "# Y_test.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN_history.history ##val_fc_mean_squared_error val_fc_accuracy' val_dense_3_mean_squared_error val_dense_3_accuracy\n",
    "\n",
    "###fc_mean_squared_error fc_accuracy dense_3_mean_squared_error dense_3_accuracy\n",
    "hist =pd.DataFrame(CNN_history.history)\n",
    "hist['epoch'] = hist.index = hist.index\n",
    "hist\n",
    "plot_name = NETWORK+'_'+str( transfer_model.__name__)+'_'+str(X_res)\n",
    "if  CLA == True and REG== False:\n",
    "    cla_acc = CNN_history.history['accuracy']\n",
    "    val_cla_acc = CNN_history.history['val_accuracy']\n",
    "    cla_loss =CNN_history.history['loss']\n",
    "    val_cla_loss = CNN_history.history['val_loss']\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    # plt.subplot(2, 2, 1)\n",
    "    plt.plot(hist['epoch'], cla_acc, label='cla-Training Accuracy')\n",
    "    plt.plot(hist['epoch'], val_cla_acc, label='cla-Validation Accuracy')\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Classification Training and Validation Accuracy')\n",
    "    plt.savefig('figures/'+plot_name+'_c.png')\n",
    "    plt.show()\n",
    "    \n",
    "if REG == True and CLA == False:\n",
    "    ## For Regression\n",
    "    reg_mse = CNN_history.history['mean_squared_error']\n",
    "    val_reg_mse =CNN_history.history['val_mean_squared_error']\n",
    "    reg_loss =CNN_history.history['loss']\n",
    "    val_reg_loss = CNN_history.history['val_loss']\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    # plt.subplot(2, 2, 1)\n",
    "    # plt.ylim(0,50000)\n",
    "    plt.plot(hist['epoch'], reg_mse, label='reg-Training MSE')\n",
    "    plt.plot(hist['epoch'], val_reg_mse, label='reg-Validation MSE')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Regression Training and Validation MSE')\n",
    "    plt.show()\n",
    "    plt.savefig('figures/'+plot_name+'_r.png')\n",
    "\n",
    "# if REG == True and CLA == True:\n",
    "#     ## For classification\n",
    "#     cla_acc = CNN_history.history['cla_accuracy']\n",
    "#     val_cla_acc = CNN_history.history['val_cla_accuracy']\n",
    "#     cla_loss =CNN_history.history['cla_loss']\n",
    "#     val_cla_loss = CNN_history.history['val_cla_loss']\n",
    "\n",
    "#     ## For Regression\n",
    "#     reg_mse = CNN_history.history['reg_mean_squared_error']\n",
    "#     val_reg_mse =CNN_history.history['val_reg_mean_squared_error']\n",
    "#     reg_loss =CNN_history.history['reg_loss']\n",
    "#     val_reg_loss = CNN_history.history['val_reg_loss']\n",
    "    \n",
    "#     plt.figure(figsize=(15, 15))\n",
    "#     plt.subplot(2, 2, 1)\n",
    "#     plt.plot(hist['epoch'], cla_acc, label='cla-Training Accuracy')\n",
    "#     plt.plot(hist['epoch'], val_cla_acc, label='cla-Validation Accuracy')\n",
    "#     plt.legend(loc='lower right')\n",
    "#     plt.title('Classification Training and Validation Accuracy')\n",
    "\n",
    "#     plt.subplot(2, 2, 2)\n",
    "#     plt.plot(hist['epoch'], reg_mse, label='reg-Training MSE')\n",
    "#     plt.plot(hist['epoch'], val_reg_mse, label='reg-Validation MSE')\n",
    "#     plt.legend(loc='upper right')\n",
    "#     plt.title('Regression Training and Validation MSE')\n",
    "#     plt.savefig('figures/'+plot_name+'_cr.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DPNNet-RT_V3.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ML_env_2",
   "language": "python",
   "name": "ml_env_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
